{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Project .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Pz9NLKJvfj6z",
        "0Xi9VuwanrXh",
        "owNfvPhKnjJS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnPNmUGa97V4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4499da69-175e-4f67-e069-88d3a5e82e8e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivdPoOVlUXBm"
      },
      "source": [
        "#Preprocessing and splitting the different datasets#\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gh9Bm5LUgYJ"
      },
      "source": [
        "\"\"\"\n",
        "Splitting the data\n",
        "\"\"\"\n",
        "from keras.datasets import cifar10\n",
        "from keras.datasets import cifar100\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# iDataset = {0, 1, ... , 19} , 20 possible sub datasets\n",
        "def splitSubSetData(iDataset):\n",
        "\n",
        "  if iDataset < 2:\n",
        "    dataset_name = \"cifar10\"\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    numDiv = 2\n",
        "\n",
        "  elif iDataset < 4:\n",
        "    dataset_name = \"mnist\"\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    iDataset = iDataset - 2\n",
        "    numDiv = 2\n",
        " \n",
        "  else:\n",
        "    dataset_name = \"cifar100\"\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    numDiv = 16\n",
        "    iDataset = iDataset - 4\n",
        "\n",
        "  numClasses = int(max(y_train) + 1)\n",
        "  sub_y_range = list(range(int(numClasses / numDiv * iDataset), int(numClasses / numDiv * (iDataset + 1))))\n",
        "\n",
        "  X = np.concatenate((x_train,x_test), axis=0)\n",
        "  y = np.concatenate((y_train,y_test), axis=0)\n",
        "  \n",
        "  indices = []\n",
        "  for i in range(len(y)):\n",
        "    if y[i] <= sub_y_range[-1] and y[i] >= sub_y_range[0]:\n",
        "      indices.append(i)\n",
        "\n",
        "  y_out = y[indices]\n",
        "  x_out = X[indices]  \n",
        "\n",
        "  # x_train, x_test,y_train,y_test=train_test_split(x_out,y_out,train_size=0.8)\n",
        "  # return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "  return x_out, y_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz9NLKJvfj6z"
      },
      "source": [
        "#Stage 1 - FixMach Algorithm implementation#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJN-bn3hUJVH"
      },
      "source": [
        "Utility functions for the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3PyAGPzgnSm"
      },
      "source": [
        "# libml\\utils.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Utilities.\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from absl import flags, logging\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "_GPUS = None\n",
        "FLAGS = flags.FLAGS\n",
        "# flags.DEFINE_bool('log_device_placement', False, 'For debugging purpose.')\n",
        "\n",
        "\n",
        "class EasyDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(EasyDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def get_config():\n",
        "    config = tf.ConfigProto()\n",
        "    if len(get_available_gpus()) > 1:\n",
        "        config.allow_soft_placement = True\n",
        "    if FLAGS.log_device_placement:\n",
        "        config.log_device_placement = True\n",
        "    config.gpu_options.allow_growth = True\n",
        "    return config\n",
        "\n",
        "\n",
        "def setup_main():\n",
        "    pass\n",
        "\n",
        "\n",
        "def setup_tf():\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    # logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "\n",
        "def smart_shape(x):\n",
        "    s = x.shape\n",
        "    st = tf.shape(x)\n",
        "    return [s[i] if s[i].value is not None else st[i] for i in range(4)]\n",
        "\n",
        "\n",
        "def ilog2(x):\n",
        "    \"\"\"Integer log2.\"\"\"\n",
        "    return int(np.ceil(np.log2(x)))\n",
        "\n",
        "\n",
        "def find_latest_checkpoint(dir, glob_term='model.ckpt-*.meta'):\n",
        "    \"\"\"Replacement for tf.train.latest_checkpoint.\n",
        "\n",
        "    It does not rely on the \"checkpoint\" file which sometimes contains\n",
        "    absolute path and is generally hard to work with when sharing files\n",
        "    between users / computers.\n",
        "    \"\"\"\n",
        "    r_step = re.compile('.*model\\.ckpt-(?P<step>\\d+)\\.meta')\n",
        "    matches = tf.gfile.Glob(os.path.join(dir, glob_term))\n",
        "    matches = [(int(r_step.match(x).group('step')), x) for x in matches]\n",
        "    ckpt_file = max(matches)[1][:-5]\n",
        "    return ckpt_file\n",
        "\n",
        "\n",
        "def get_latest_global_step(dir):\n",
        "    \"\"\"Loads the global step from the latest checkpoint in directory.\n",
        "\n",
        "    Args:\n",
        "      dir: string, path to the checkpoint directory.\n",
        "\n",
        "    Returns:\n",
        "      int, the global step of the latest checkpoint or 0 if none was found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        checkpoint_reader = tf.train.NewCheckpointReader(find_latest_checkpoint(dir))\n",
        "        return checkpoint_reader.get_tensor(tf.GraphKeys.GLOBAL_STEP)\n",
        "    except:  # pylint: disable=bare-except\n",
        "        return 0\n",
        "\n",
        "\n",
        "def get_latest_global_step_in_subdir(dir):\n",
        "    \"\"\"Loads the global step from the latest checkpoint in sub-directories.\n",
        "\n",
        "    Args:\n",
        "      dir: string, parent of the checkpoint directories.\n",
        "\n",
        "    Returns:\n",
        "      int, the global step of the latest checkpoint or 0 if none was found.\n",
        "    \"\"\"\n",
        "    sub_dirs = (x for x in tf.gfile.Glob(os.path.join(dir, '*')) if os.path.isdir(x))\n",
        "    step = 0\n",
        "    for x in sub_dirs:\n",
        "        step = max(step, get_latest_global_step(x))\n",
        "    return step\n",
        "\n",
        "\n",
        "def getter_ema(ema, getter, name, *args, **kwargs):\n",
        "    \"\"\"Exponential moving average getter for variable scopes.\n",
        "\n",
        "    Args:\n",
        "        ema: ExponentialMovingAverage object, where to get variable moving averages.\n",
        "        getter: default variable scope getter.\n",
        "        name: variable name.\n",
        "        *args: extra args passed to default getter.\n",
        "        **kwargs: extra args passed to default getter.\n",
        "\n",
        "    Returns:\n",
        "        If found the moving average variable, otherwise the default variable.\n",
        "    \"\"\"\n",
        "    var = getter(name, *args, **kwargs)\n",
        "    ema_var = ema.average(var)\n",
        "    return ema_var if ema_var else var\n",
        "\n",
        "\n",
        "def model_vars(scope=None):\n",
        "    return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
        "\n",
        "\n",
        "def gpu(x):\n",
        "    return '/gpu:%d' % (x % max(1, len(get_available_gpus())))\n",
        "\n",
        "\n",
        "def get_available_gpus():\n",
        "    global _GPUS\n",
        "    if _GPUS is None:\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        local_device_protos = device_lib.list_local_devices(session_config=config)\n",
        "        _GPUS = tuple([x.name for x in local_device_protos if x.device_type == 'GPU'])\n",
        "    return _GPUS\n",
        "\n",
        "\n",
        "def get_gpu():\n",
        "    gpus = get_available_gpus()\n",
        "    pos = 0\n",
        "    while 1:\n",
        "        yield gpus[pos]\n",
        "        pos = (pos + 1) % len(gpus)\n",
        "\n",
        "\n",
        "def average_gradients(tower_grads):\n",
        "    \"\"\"\n",
        "    Calculate the average gradient for each shared variable across all towers.\n",
        "    Note that this function provides a synchronization point across all towers.\n",
        "    Args:\n",
        "      tower_grads: List of lists of (gradient, variable) tuples. For each tower, a list of its gradients.\n",
        "    Returns:\n",
        "       List of pairs of (gradient, variable) where the gradient has been averaged\n",
        "       across all towers.\n",
        "    \"\"\"\n",
        "    if len(tower_grads) <= 1:\n",
        "        return tower_grads[0]\n",
        "\n",
        "    average_grads = []\n",
        "    for grads_and_vars in zip(*tower_grads):\n",
        "        grad = tf.reduce_mean([gv[0] for gv in grads_and_vars], 0)\n",
        "        average_grads.append((grad, grads_and_vars[0][1]))\n",
        "    return average_grads\n",
        "\n",
        "\n",
        "def para_list(fn, *args):\n",
        "    \"\"\"Run on multiple GPUs in parallel and return list of results.\"\"\"\n",
        "    gpus = len(get_available_gpus())\n",
        "    if gpus <= 1:\n",
        "        return zip(*[fn(*args)])\n",
        "    splitted = [tf.split(x, gpus) for x in args]\n",
        "    outputs = []\n",
        "    for gpu, x in enumerate(zip(*splitted)):\n",
        "        with tf.name_scope('tower%d' % gpu):\n",
        "            with tf.device(tf.train.replica_device_setter(\n",
        "                    worker_device='/gpu:%d' % gpu, ps_device='/cpu:0', ps_tasks=1)):\n",
        "                outputs.append(fn(*x))\n",
        "    return zip(*outputs)\n",
        "\n",
        "\n",
        "def para_mean(fn, *args):\n",
        "    \"\"\"Run on multiple GPUs in parallel and return means.\"\"\"\n",
        "    gpus = len(get_available_gpus())\n",
        "    if gpus <= 1:\n",
        "        return fn(*args)\n",
        "    splitted = [tf.split(x, gpus) for x in args]\n",
        "    outputs = []\n",
        "    for gpu, x in enumerate(zip(*splitted)):\n",
        "        with tf.name_scope('tower%d' % gpu):\n",
        "            with tf.device(tf.train.replica_device_setter(\n",
        "                    worker_device='/gpu:%d' % gpu, ps_device='/cpu:0', ps_tasks=1)):\n",
        "                outputs.append(fn(*x))\n",
        "    if isinstance(outputs[0], (tuple, list)):\n",
        "        return [tf.reduce_mean(x, 0) for x in zip(*outputs)]\n",
        "    return tf.reduce_mean(outputs, 0)\n",
        "\n",
        "\n",
        "def para_cat(fn, *args):\n",
        "    \"\"\"Run on multiple GPUs in parallel and return concatenated outputs.\"\"\"\n",
        "    gpus = len(get_available_gpus())\n",
        "    if gpus <= 1:\n",
        "        return fn(*args)\n",
        "    splitted = [tf.split(x, gpus) for x in args]\n",
        "    outputs = []\n",
        "    for gpu, x in enumerate(zip(*splitted)):\n",
        "        with tf.name_scope('tower%d' % gpu):\n",
        "            with tf.device(tf.train.replica_device_setter(\n",
        "                    worker_device='/gpu:%d' % gpu, ps_device='/cpu:0', ps_tasks=1)):\n",
        "                outputs.append(fn(*x))\n",
        "    if isinstance(outputs[0], (tuple, list)):\n",
        "        return [tf.concat(x, axis=0) for x in zip(*outputs)]\n",
        "    return tf.concat(outputs, axis=0)\n",
        "\n",
        "\n",
        "def interleave(x, batch):\n",
        "    s = x.get_shape().as_list()\n",
        "    return tf.reshape(tf.transpose(tf.reshape(x, [-1, batch] + s[1:]), [1, 0] + list(range(2, 1+len(s)))), [-1] + s[1:])\n",
        "\n",
        "\n",
        "def de_interleave(x, batch):\n",
        "    s = x.get_shape().as_list()\n",
        "    return tf.reshape(tf.transpose(tf.reshape(x, [batch, -1] + s[1:]), [1, 0] + list(range(2, 1+len(s)))), [-1] +s[1:])\n",
        "\n",
        "\n",
        "def combine_dicts(*args):\n",
        "    # Python 2 compatible way to combine several dictionaries\n",
        "    # We need it because currently TPU code does not work with python 3\n",
        "    result = {}\n",
        "    for d in args:\n",
        "        result.update(d)\n",
        "    return result\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xi9VuwanrXh"
      },
      "source": [
        "The original method to load the data from the GitHub project\n",
        "---\n",
        "-we used another method, \n",
        "described above using an index for sub dataset, \n",
        "iDataset = {0 : 19}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5Js0bjVADO7"
      },
      "source": [
        "# create_datasets.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "# Copyright 2018 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Script to download all datasets and create .tfrecord files.\n",
        "\"\"\"\n",
        "\n",
        "import collections\n",
        "import gzip\n",
        "import os\n",
        "import tarfile\n",
        "import tempfile\n",
        "from urllib import request\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import tensorflow as tf\n",
        "from absl import app\n",
        "from tqdm import trange\n",
        "\n",
        "# from libml import data as libml_data\n",
        "# from libml.utils import EasyDict\n",
        "\n",
        "URLS = {\n",
        "    'svhn': 'http://ufldl.stanford.edu/housenumbers/{}_32x32.mat',\n",
        "    'cifar10': 'https://www.cs.toronto.edu/~kriz/cifar-10-matlab.tar.gz',\n",
        "    'cifar100': 'https://www.cs.toronto.edu/~kriz/cifar-100-matlab.tar.gz',\n",
        "    'stl10': 'http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz',\n",
        "}\n",
        "\n",
        "\n",
        "def _encode_png(images):\n",
        "    raw = []\n",
        "    with tf.Session() as sess, tf.device('cpu:0'):\n",
        "        image_x = tf.placeholder(tf.uint8, [None, None, None], 'image_x')\n",
        "        to_png = tf.image.encode_png(image_x)\n",
        "        for x in trange(images.shape[0], desc='PNG Encoding', leave=False):\n",
        "            raw.append(sess.run(to_png, feed_dict={image_x: images[x]}))\n",
        "    return raw\n",
        "\n",
        "\n",
        "def _load_svhn():\n",
        "    splits = collections.OrderedDict()\n",
        "    for split in ['train', 'test', 'extra']:\n",
        "        with tempfile.NamedTemporaryFile() as f:\n",
        "            request.urlretrieve(URLS['svhn'].format(split), f.name)\n",
        "            data_dict = scipy.io.loadmat(f.name)\n",
        "        dataset = {}\n",
        "        dataset['images'] = np.transpose(data_dict['X'], [3, 0, 1, 2])\n",
        "        dataset['images'] = _encode_png(dataset['images'])\n",
        "        dataset['labels'] = data_dict['y'].reshape((-1))\n",
        "        # SVHN raw data uses labels from 1 to 10; use 0 to 9 instead.\n",
        "        dataset['labels'] -= 1\n",
        "        splits[split] = dataset\n",
        "    return splits\n",
        "\n",
        "\n",
        "def _load_stl10():\n",
        "    def unflatten(images):\n",
        "        return np.transpose(images.reshape((-1, 3, 96, 96)),\n",
        "                            [0, 3, 2, 1])\n",
        "\n",
        "    with tempfile.NamedTemporaryFile() as f:\n",
        "        if tf.gfile.Exists('stl10/stl10_binary.tar.gz'):\n",
        "            f = tf.gfile.Open('stl10/stl10_binary.tar.gz', 'rb')\n",
        "        else:\n",
        "            request.urlretrieve(URLS['stl10'], f.name)\n",
        "        tar = tarfile.open(fileobj=f)\n",
        "        train_X = tar.extractfile('stl10_binary/train_X.bin')\n",
        "        train_y = tar.extractfile('stl10_binary/train_y.bin')\n",
        "\n",
        "        test_X = tar.extractfile('stl10_binary/test_X.bin')\n",
        "        test_y = tar.extractfile('stl10_binary/test_y.bin')\n",
        "\n",
        "        unlabeled_X = tar.extractfile('stl10_binary/unlabeled_X.bin')\n",
        "\n",
        "        train_set = {'images': np.frombuffer(train_X.read(), dtype=np.uint8),\n",
        "                     'labels': np.frombuffer(train_y.read(), dtype=np.uint8) - 1}\n",
        "\n",
        "        test_set = {'images': np.frombuffer(test_X.read(), dtype=np.uint8),\n",
        "                    'labels': np.frombuffer(test_y.read(), dtype=np.uint8) - 1}\n",
        "\n",
        "        _imgs = np.frombuffer(unlabeled_X.read(), dtype=np.uint8)\n",
        "        unlabeled_set = {'images': _imgs,\n",
        "                         'labels': np.zeros(100000, dtype=np.uint8)}\n",
        "\n",
        "        fold_indices = tar.extractfile('stl10_binary/fold_indices.txt').read()\n",
        "\n",
        "    train_set['images'] = _encode_png(unflatten(train_set['images']))\n",
        "    test_set['images'] = _encode_png(unflatten(test_set['images']))\n",
        "    unlabeled_set['images'] = _encode_png(unflatten(unlabeled_set['images']))\n",
        "    return dict(train=train_set, test=test_set, unlabeled=unlabeled_set,\n",
        "                files=[EasyDict(filename=\"stl10_fold_indices.txt\", data=fold_indices)])\n",
        "\n",
        "\n",
        "def _load_cifar10():\n",
        "    def unflatten(images):\n",
        "        return np.transpose(images.reshape((images.shape[0], 3, 32, 32)),\n",
        "                            [0, 2, 3, 1])\n",
        "\n",
        "    with tempfile.NamedTemporaryFile() as f:\n",
        "        request.urlretrieve(URLS['cifar10'], f.name)\n",
        "        tar = tarfile.open(fileobj=f)\n",
        "        train_data_batches, train_data_labels = [], []\n",
        "        for batch in range(1, 6):\n",
        "            data_dict = scipy.io.loadmat(tar.extractfile(\n",
        "                'cifar-10-batches-mat/data_batch_{}.mat'.format(batch)))\n",
        "            train_data_batches.append(data_dict['data'])\n",
        "            train_data_labels.append(data_dict['labels'].flatten())\n",
        "        train_set = {'images': np.concatenate(train_data_batches, axis=0),\n",
        "                     'labels': np.concatenate(train_data_labels, axis=0)}\n",
        "        data_dict = scipy.io.loadmat(tar.extractfile(\n",
        "            'cifar-10-batches-mat/test_batch.mat'))\n",
        "        test_set = {'images': data_dict['data'],\n",
        "                    'labels': data_dict['labels'].flatten()}\n",
        "    train_set['images'] = _encode_png(unflatten(train_set['images']))\n",
        "    test_set['images'] = _encode_png(unflatten(test_set['images']))\n",
        "    return dict(train=train_set, test=test_set)\n",
        "    # the output is a dictionary{train, test} of dictionaries{images,labels} of ndarray's \n",
        "\n",
        "\n",
        "def _load_cifar100():\n",
        "    def unflatten(images):\n",
        "        return np.transpose(images.reshape((images.shape[0], 3, 32, 32)),\n",
        "                            [0, 2, 3, 1])\n",
        "\n",
        "    with tempfile.NamedTemporaryFile() as f:\n",
        "        request.urlretrieve(URLS['cifar100'], f.name)\n",
        "        tar = tarfile.open(fileobj=f)\n",
        "        data_dict = scipy.io.loadmat(tar.extractfile('cifar-100-matlab/train.mat'))\n",
        "        train_set = {'images': data_dict['data'],\n",
        "                     'labels': data_dict['fine_labels'].flatten()}\n",
        "        data_dict = scipy.io.loadmat(tar.extractfile('cifar-100-matlab/test.mat'))\n",
        "        test_set = {'images': data_dict['data'],\n",
        "                    'labels': data_dict['fine_labels'].flatten()}\n",
        "    train_set['images'] = _encode_png(unflatten(train_set['images']))\n",
        "    test_set['images'] = _encode_png(unflatten(test_set['images']))\n",
        "    return dict(train=train_set, test=test_set)\n",
        "\n",
        "\n",
        "def _load_fashionmnist():\n",
        "    def _read32(data):\n",
        "        dt = np.dtype(np.uint32).newbyteorder('>')\n",
        "        return np.frombuffer(data.read(4), dtype=dt)[0]\n",
        "\n",
        "    image_filename = '{}-images-idx3-ubyte'\n",
        "    label_filename = '{}-labels-idx1-ubyte'\n",
        "    split_files = [('train', 'train'), ('test', 't10k')]\n",
        "    splits = {}\n",
        "    for split, split_file in split_files:\n",
        "        with tempfile.NamedTemporaryFile() as f:\n",
        "            request.urlretrieve(URLS['fashion_mnist'].format(image_filename.format(split_file)), f.name)\n",
        "            with gzip.GzipFile(fileobj=f, mode='r') as data:\n",
        "                assert _read32(data) == 2051\n",
        "                n_images = _read32(data)\n",
        "                row = _read32(data)\n",
        "                col = _read32(data)\n",
        "                images = np.frombuffer(data.read(n_images * row * col), dtype=np.uint8)\n",
        "                images = images.reshape((n_images, row, col, 1))\n",
        "        with tempfile.NamedTemporaryFile() as f:\n",
        "            request.urlretrieve(URLS['fashion_mnist'].format(label_filename.format(split_file)), f.name)\n",
        "            with gzip.GzipFile(fileobj=f, mode='r') as data:\n",
        "                assert _read32(data) == 2049\n",
        "                n_labels = _read32(data)\n",
        "                labels = np.frombuffer(data.read(n_labels), dtype=np.uint8)\n",
        "        splits[split] = {'images': _encode_png(images), 'labels': labels}\n",
        "    return splits\n",
        "\n",
        "\n",
        "def _int64_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "\n",
        "def _save_as_tfrecord(data, filename):\n",
        "    assert len(data['images']) == len(data['labels'])\n",
        "    filename = os.path.join(libml_data.DATA_DIR, filename + '.tfrecord')\n",
        "    print('Saving dataset:', filename)\n",
        "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
        "        for x in trange(len(data['images']), desc='Building records'):\n",
        "            feat = dict(image=_bytes_feature(data['images'][x]),\n",
        "                        label=_int64_feature(data['labels'][x]))\n",
        "            record = tf.train.Example(features=tf.train.Features(feature=feat))\n",
        "            writer.write(record.SerializeToString())\n",
        "    print('Saved:', filename)\n",
        "\n",
        "\n",
        "def _is_installed(name, checksums):\n",
        "    for subset, checksum in checksums.items():\n",
        "        filename = os.path.join(libml_data.DATA_DIR, '%s-%s.tfrecord' % (name, subset))\n",
        "        if not tf.gfile.Exists(filename):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def _save_files(files, *args, **kwargs):\n",
        "    del args, kwargs\n",
        "    for folder in frozenset(os.path.dirname(x) for x in files):\n",
        "        tf.gfile.MakeDirs(os.path.join(libml_data.DATA_DIR, folder))\n",
        "    for filename, contents in files.items():\n",
        "        with tf.gfile.Open(os.path.join(libml_data.DATA_DIR, filename), 'w') as f:\n",
        "            f.write(contents)\n",
        "\n",
        "\n",
        "def _is_installed_folder(name, folder):\n",
        "    return tf.gfile.Exists(os.path.join(libml_data.DATA_DIR, name, folder))\n",
        "\n",
        "\n",
        "CONFIGS = dict(\n",
        "    cifar10=dict(loader=_load_cifar10, checksums=dict(train=None, test=None)),\n",
        "    cifar100=dict(loader=_load_cifar100, checksums=dict(train=None, test=None)),\n",
        "    svhn=dict(loader=_load_svhn, checksums=dict(train=None, test=None, extra=None)),\n",
        "    stl10=dict(loader=_load_stl10, checksums=dict(train=None, test=None)),\n",
        ")\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "    if len(argv[1:]):\n",
        "        subset = set(argv[1:])\n",
        "    else:\n",
        "        subset = set(CONFIGS.keys())\n",
        "    tf.gfile.MakeDirs(DATA_DIR)\n",
        "    for name, config in CONFIGS.items():\n",
        "        if name not in subset:\n",
        "            continue\n",
        "        if 'is_installed' in config:\n",
        "            if config['is_installed']():\n",
        "                print('Skipping already installed:', name)\n",
        "                continue\n",
        "        elif _is_installed(name, config['checksums']):\n",
        "            print('Skipping already installed:', name)\n",
        "            continue\n",
        "        print('Preparing', name)\n",
        "        datas = config['loader']()\n",
        "        saver = config.get('saver', _save_as_tfrecord)\n",
        "        for sub_name, data in datas.items():\n",
        "            if sub_name == 'readme':\n",
        "                filename = os.path.join(libml_data.DATA_DIR, '%s-%s.txt' % (name, sub_name))\n",
        "                with tf.gfile.Open(filename, 'w') as f:\n",
        "                    f.write(data)\n",
        "            elif sub_name == 'files':\n",
        "                for file_and_data in data:\n",
        "                    path = os.path.join(libml_data.DATA_DIR, file_and_data.filename)\n",
        "                    with tf.gfile.Open(path, \"wb\") as f:\n",
        "                        f.write(file_and_data.data)\n",
        "            else:\n",
        "                saver(data, '%s-%s' % (name, sub_name))\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "    # app.run(main)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owNfvPhKnjJS"
      },
      "source": [
        "Stage 1 & Stage 2 - Adding augmentation methods as hyperparameter\n",
        "---\n",
        "Augmentation methods, used for the improvement of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0FKd_rDlBND"
      },
      "source": [
        "# libml\\ctaugment.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Control Theory based self-augmentation.\"\"\"\n",
        "import random\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageEnhance, ImageFilter\n",
        "\n",
        "OPS = {}\n",
        "OP = namedtuple('OP', ('f', 'bins'))\n",
        "Sample = namedtuple('Sample', ('train', 'probe'))\n",
        "\n",
        "\n",
        "def register(*bins):\n",
        "    def wrap(f):\n",
        "        OPS[f.__name__] = OP(f, bins)\n",
        "        return f\n",
        "\n",
        "    return wrap\n",
        "\n",
        "\n",
        "def apply(x, ops):\n",
        "    if ops is None:\n",
        "        return x\n",
        "    y = Image.fromarray(np.round(127.5 * (1 + x)).clip(0, 255).astype('uint8'))\n",
        "    for op, args in ops:\n",
        "        y = OPS[op].f(y, *args)\n",
        "    return np.asarray(y).astype('f') / 127.5 - 1\n",
        "\n",
        "\n",
        "class CTAugment:\n",
        "    def __init__(self, depth=2, th=0.85, decay=0.99):\n",
        "        self.decay = decay\n",
        "        self.depth = depth\n",
        "        self.th = th\n",
        "        self.rates = {}\n",
        "        for k, op in OPS.items():\n",
        "            self.rates[k] = tuple([np.ones(x, 'f') for x in op.bins])\n",
        "\n",
        "    def rate_to_p(self, rate):\n",
        "        p = rate + (1 - self.decay)  # Avoid to have all zero.\n",
        "        p = p / p.max()\n",
        "        p[p < self.th] = 0\n",
        "        return p\n",
        "\n",
        "    def policy(self, probe):\n",
        "        kl = list(OPS.keys())\n",
        "        v = []\n",
        "        if probe:\n",
        "            for _ in range(self.depth):\n",
        "                k = random.choice(kl)\n",
        "                bins = self.rates[k]\n",
        "                rnd = np.random.uniform(0, 1, len(bins))\n",
        "                v.append(OP(k, rnd.tolist()))\n",
        "            return v\n",
        "        for _ in range(self.depth):\n",
        "            vt = []\n",
        "            k = random.choice(kl)\n",
        "            bins = self.rates[k]\n",
        "            rnd = np.random.uniform(0, 1, len(bins))\n",
        "            for r, bin in zip(rnd, bins):\n",
        "                p = self.rate_to_p(bin)\n",
        "                value = np.random.choice(p.shape[0], p=p / p.sum())\n",
        "                vt.append((value + r) / p.shape[0])\n",
        "            v.append(OP(k, vt))\n",
        "        return v\n",
        "\n",
        "    def update_rates(self, policy, proximity):\n",
        "        for k, bins in policy:\n",
        "            for p, rate in zip(bins, self.rates[k]):\n",
        "                p = int(p * len(rate) * 0.999)\n",
        "                rate[p] = rate[p] * self.decay + proximity * (1 - self.decay)\n",
        "\n",
        "    def stats(self):\n",
        "        return '\\n'.join('%-16s    %s' % (k, ' / '.join(' '.join('%.2f' % x for x in self.rate_to_p(rate))\n",
        "                                                        for rate in self.rates[k]))\n",
        "                         for k in sorted(OPS.keys()))\n",
        "\n",
        "\n",
        "def _enhance(x, op, level):\n",
        "    return op(x).enhance(0.1 + 1.9 * level)\n",
        "\n",
        "\n",
        "def _imageop(x, op, level):\n",
        "    return Image.blend(x, op(x), level)\n",
        "\n",
        "\n",
        "def _filter(x, op, level):\n",
        "    return Image.blend(x, x.filter(op), level)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def autocontrast(x, level):\n",
        "    return _imageop(x, ImageOps.autocontrast, level)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def blur(x, level):\n",
        "    return _filter(x, ImageFilter.BLUR, level)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def brightness(x, brightness):\n",
        "    return _enhance(x, ImageEnhance.Brightness, brightness)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def color(x, color):\n",
        "    return _enhance(x, ImageEnhance.Color, color)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def contrast(x, contrast):\n",
        "    return _enhance(x, ImageEnhance.Contrast, contrast)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def cutout(x, level):\n",
        "    \"\"\"Apply cutout to pil_img at the specified level.\"\"\"\n",
        "    size = 1 + int(level * min(x.size) * 0.499)\n",
        "    img_height, img_width = x.size\n",
        "    height_loc = np.random.randint(low=0, high=img_height)\n",
        "    width_loc = np.random.randint(low=0, high=img_width)\n",
        "    upper_coord = (max(0, height_loc - size // 2), max(0, width_loc - size // 2))\n",
        "    lower_coord = (min(img_height, height_loc + size // 2), min(img_width, width_loc + size // 2))\n",
        "    pixels = x.load()  # create the pixel map\n",
        "    for i in range(upper_coord[0], lower_coord[0]):  # for every col:\n",
        "        for j in range(upper_coord[1], lower_coord[1]):  # For every row\n",
        "            pixels[i, j] = (127, 127, 127)  # set the color accordingly\n",
        "    return x\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def equalize(x, level):\n",
        "    return _imageop(x, ImageOps.equalize, level)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def invert(x, level):\n",
        "    return _imageop(x, ImageOps.invert, level)\n",
        "\n",
        "\n",
        "@register()\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "@register(8)\n",
        "def posterize(x, level):\n",
        "    level = 1 + int(level * 7.999)\n",
        "    return ImageOps.posterize(x, level)\n",
        "\n",
        "\n",
        "@register(17, 6)\n",
        "def rescale(x, scale, method):\n",
        "    s = x.size\n",
        "    scale *= 0.25\n",
        "    crop = (scale * s[0], scale * s[1], s[0] * (1 - scale), s[1] * (1 - scale))\n",
        "    methods = (Image.ANTIALIAS, Image.BICUBIC, Image.BILINEAR, Image.BOX, Image.HAMMING, Image.NEAREST)\n",
        "    method = methods[int(method * 5.99)]\n",
        "    return x.crop(crop).resize(x.size, method)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def rotate(x, angle):\n",
        "    angle = int(np.round((2 * angle - 1) * 45))\n",
        "    return x.rotate(angle)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def sharpness(x, sharpness):\n",
        "    return _enhance(x, ImageEnhance.Sharpness, sharpness)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def shear_x(x, shear):\n",
        "    shear = (2 * shear - 1) * 0.3\n",
        "    return x.transform(x.size, Image.AFFINE, (1, shear, 0, 0, 1, 0))\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def shear_y(x, shear):\n",
        "    shear = (2 * shear - 1) * 0.3\n",
        "    return x.transform(x.size, Image.AFFINE, (1, 0, 0, shear, 1, 0))\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def smooth(x, level):\n",
        "    return _filter(x, ImageFilter.SMOOTH, level)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def solarize(x, th):\n",
        "    th = int(th * 255.999)\n",
        "    return ImageOps.solarize(x, th)\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def translate_x(x, delta):\n",
        "    delta = (2 * delta - 1) * 0.3\n",
        "    return x.transform(x.size, Image.AFFINE, (1, 0, delta, 0, 1, 0))\n",
        "\n",
        "\n",
        "@register(17)\n",
        "def translate_y(x, delta):\n",
        "    delta = (2 * delta - 1) * 0.3\n",
        "    return x.transform(x.size, Image.AFFINE, (1, 0, 0, 0, 1, delta))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L84I3yx0pGqw"
      },
      "source": [
        "Third party augmentation methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ooHrZ1lnQL"
      },
      "source": [
        "# third_party\\auto_augment\\augmentations.py\n",
        "# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Transforms used in the Augmentation Policies.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "# pylint:disable=g-multiple-import\n",
        "from PIL import ImageOps, ImageEnhance, ImageFilter, Image\n",
        "\n",
        "# pylint:enable=g-multiple-import\n",
        "\n",
        "\n",
        "# IMAGE_SIZE = 32\n",
        "# What is the dataset mean and std of the images on the training set\n",
        "# MEANS = [0.49139968, 0.48215841, 0.44653091]\n",
        "# STDS = [0.24703223, 0.24348513, 0.26158784]\n",
        "PARAMETER_MAX = 10  # What is the max 'level' a transform could be predicted\n",
        "\n",
        "\n",
        "def random_flip(x):\n",
        "    \"\"\"Flip the input x horizontally with 50% probability.\"\"\"\n",
        "    if np.random.rand(1)[0] > 0.5:\n",
        "        return np.fliplr(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def zero_pad_and_crop(img, amount=4):\n",
        "    \"\"\"Zero pad by `amount` zero pixels on each side then take a random crop.\n",
        "\n",
        "    Args:\n",
        "      img: numpy image that will be zero padded and cropped.\n",
        "      amount: amount of zeros to pad `img` with horizontally and verically.\n",
        "\n",
        "    Returns:\n",
        "      The cropped zero padded img. The returned numpy array will be of the same\n",
        "      shape as `img`.\n",
        "    \"\"\"\n",
        "    padded_img = np.zeros((img.shape[0] + amount * 2, img.shape[1] + amount * 2,\n",
        "                           img.shape[2]))\n",
        "    padded_img[amount:img.shape[0] + amount, amount:\n",
        "                                             img.shape[1] + amount, :] = img\n",
        "    top = np.random.randint(low=0, high=2 * amount)\n",
        "    left = np.random.randint(low=0, high=2 * amount)\n",
        "    new_img = padded_img[top:top + img.shape[0], left:left + img.shape[1], :]\n",
        "    return new_img\n",
        "\n",
        "\n",
        "def create_cutout_mask(img_height, img_width, num_channels, size):\n",
        "    \"\"\"Creates a zero mask used for cutout of shape `img_height` x `img_width`.\n",
        "\n",
        "    Args:\n",
        "      img_height: Height of image cutout mask will be applied to.\n",
        "      img_width: Width of image cutout mask will be applied to.\n",
        "      num_channels: Number of channels in the image.\n",
        "      size: Size of the zeros mask.\n",
        "\n",
        "    Returns:\n",
        "      A mask of shape `img_height` x `img_width` with all ones except for a\n",
        "      square of zeros of shape `size` x `size`. This mask is meant to be\n",
        "      elementwise multiplied with the original image. Additionally returns\n",
        "      the `upper_coord` and `lower_coord` which specify where the cutout mask\n",
        "      will be applied.\n",
        "    \"\"\"\n",
        "    assert img_height == img_width\n",
        "\n",
        "    # Sample center where cutout mask will be applied\n",
        "    height_loc = np.random.randint(low=0, high=img_height)\n",
        "    width_loc = np.random.randint(low=0, high=img_width)\n",
        "\n",
        "    # Determine upper right and lower left corners of patch\n",
        "    upper_coord = (max(0, height_loc - size // 2), max(0, width_loc - size // 2))\n",
        "    lower_coord = (min(img_height, height_loc + size // 2),\n",
        "                   min(img_width, width_loc + size // 2))\n",
        "    mask_height = lower_coord[0] - upper_coord[0]\n",
        "    mask_width = lower_coord[1] - upper_coord[1]\n",
        "    assert mask_height > 0\n",
        "    assert mask_width > 0\n",
        "\n",
        "    mask = np.ones((img_height, img_width, num_channels))\n",
        "    mask[upper_coord[0]:lower_coord[0], upper_coord[1]:lower_coord[1], :] = 0\n",
        "    return mask, upper_coord, lower_coord\n",
        "\n",
        "\n",
        "def cutout_numpy(img, size=16):\n",
        "    \"\"\"Apply cutout with mask of shape `size` x `size` to `img`.\n",
        "\n",
        "    The cutout operation is from the paper https://arxiv.org/abs/1708.04552.\n",
        "    This operation applies a `size`x`size` mask of zeros to a random location\n",
        "    within `img`.\n",
        "\n",
        "    Args:\n",
        "      img: Numpy image that cutout will be applied to.\n",
        "      size: Height/width of the cutout mask that will be\n",
        "\n",
        "    Returns:\n",
        "      A numpy tensor that is the result of applying the cutout mask to `img`.\n",
        "    \"\"\"\n",
        "    if size <= 0:\n",
        "        return img\n",
        "    assert len(img.shape) == 3\n",
        "    img_height, img_width, num_channels = img.shape\n",
        "    mask = create_cutout_mask(img_height, img_width, num_channels, size)[0]\n",
        "    return img * mask\n",
        "\n",
        "\n",
        "def float_parameter(level, maxval):\n",
        "    \"\"\"Helper function to scale `val` between 0 and maxval .\n",
        "\n",
        "    Args:\n",
        "      level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
        "      maxval: Maximum value that the operation can have. This will be scaled\n",
        "        to level/PARAMETER_MAX.\n",
        "\n",
        "    Returns:\n",
        "      A float that results from scaling `maxval` according to `level`.\n",
        "    \"\"\"\n",
        "    return float(level) * maxval / PARAMETER_MAX\n",
        "\n",
        "\n",
        "def int_parameter(level, maxval):\n",
        "    \"\"\"Helper function to scale `val` between 0 and maxval .\n",
        "\n",
        "    Args:\n",
        "      level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
        "      maxval: Maximum value that the operation can have. This will be scaled\n",
        "        to level/PARAMETER_MAX.\n",
        "\n",
        "    Returns:\n",
        "      An int that results from scaling `maxval` according to `level`.\n",
        "    \"\"\"\n",
        "    return int(level * maxval / PARAMETER_MAX)\n",
        "\n",
        "\n",
        "def pil_wrap(img, mean=0.5, std=0.5):\n",
        "    \"\"\"Convert the `img` numpy tensor to a PIL Image.\"\"\"\n",
        "    return Image.fromarray(\n",
        "        np.uint8((img * std + mean) * 255.0)).convert('RGBA')\n",
        "\n",
        "\n",
        "def pil_unwrap(pil_img, mean=0.5, std=0.5):\n",
        "    \"\"\"Converts the PIL img to a numpy array.\"\"\"\n",
        "    s = pil_img.size\n",
        "    pic_array = (np.array(pil_img.getdata()).reshape((s[0], s[1], 4)) / 255.0)\n",
        "    i1, i2 = np.where(pic_array[:, :, 3] == 0)\n",
        "    pic_array = (pic_array[:, :, :3] - mean) / std\n",
        "    pic_array[i1, i2] = [0, 0, 0]\n",
        "    return pic_array\n",
        "\n",
        "\n",
        "def apply_policy(policy, img):\n",
        "    \"\"\"Apply the `policy` to the numpy `img`.\n",
        "\n",
        "    Args:\n",
        "      policy: A list of tuples with the form (name, probability, level) where\n",
        "        `name` is the name of the augmentation operation to apply, `probability`\n",
        "        is the probability of applying the operation and `level` is what strength\n",
        "        the operation to apply.\n",
        "      img: Numpy image that will have `policy` applied to it.\n",
        "\n",
        "    Returns:\n",
        "      The result of applying `policy` to `img`.\n",
        "    \"\"\"\n",
        "    pil_img = pil_wrap(img)\n",
        "    for xform in policy:\n",
        "        assert len(xform) == 3\n",
        "        name, probability, level = xform\n",
        "        xform_fn = NAME_TO_TRANSFORM[name].pil_transformer(probability, level)\n",
        "        pil_img = xform_fn(pil_img)\n",
        "    return pil_unwrap(pil_img)\n",
        "\n",
        "\n",
        "class TransformFunction(object):\n",
        "    \"\"\"Wraps the Transform function for pretty printing options.\"\"\"\n",
        "\n",
        "    def __init__(self, func, name):\n",
        "        self.f = func\n",
        "        self.name = name\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '<' + self.name + '>'\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        return self.f(pil_img)\n",
        "\n",
        "\n",
        "class TransformT(object):\n",
        "    \"\"\"Each instance of this class represents a specific transform.\"\"\"\n",
        "\n",
        "    def __init__(self, name, xform_fn):\n",
        "        self.name = name\n",
        "        self.xform = xform_fn\n",
        "\n",
        "    def pil_transformer(self, probability, level):\n",
        "        def return_function(im):\n",
        "            if random.random() < probability:\n",
        "                im = self.xform(im, level)\n",
        "            return im\n",
        "\n",
        "        name = self.name + '({:.1f},{})'.format(probability, level)\n",
        "        return TransformFunction(return_function, name)\n",
        "\n",
        "    def do_transform(self, image, level):\n",
        "        f = self.pil_transformer(PARAMETER_MAX, level)\n",
        "        return pil_unwrap(f(pil_wrap(image)))\n",
        "\n",
        "\n",
        "################## Transform Functions ##################\n",
        "identity = TransformT('Identity', lambda pil_img, level: pil_img)\n",
        "flip_lr = TransformT(\n",
        "    'FlipLR',\n",
        "    lambda pil_img, level: pil_img.transpose(Image.FLIP_LEFT_RIGHT))\n",
        "flip_ud = TransformT(\n",
        "    'FlipUD',\n",
        "    lambda pil_img, level: pil_img.transpose(Image.FLIP_TOP_BOTTOM))\n",
        "# pylint:disable=g-long-lambda\n",
        "auto_contrast = TransformT(\n",
        "    'AutoContrast',\n",
        "    lambda pil_img, level: ImageOps.autocontrast(\n",
        "        pil_img.convert('RGB')).convert('RGBA'))\n",
        "equalize = TransformT(\n",
        "    'Equalize',\n",
        "    lambda pil_img, level: ImageOps.equalize(\n",
        "        pil_img.convert('RGB')).convert('RGBA'))\n",
        "invert = TransformT(\n",
        "    'Invert',\n",
        "    lambda pil_img, level: ImageOps.invert(\n",
        "        pil_img.convert('RGB')).convert('RGBA'))\n",
        "# pylint:enable=g-long-lambda\n",
        "blur = TransformT(\n",
        "    'Blur', lambda pil_img, level: pil_img.filter(ImageFilter.BLUR))\n",
        "smooth = TransformT(\n",
        "    'Smooth',\n",
        "    lambda pil_img, level: pil_img.filter(ImageFilter.SMOOTH))\n",
        "\n",
        "\n",
        "def _rotate_impl(pil_img, level):\n",
        "    \"\"\"Rotates `pil_img` from -30 to 30 degrees depending on `level`.\"\"\"\n",
        "    degrees = int_parameter(level, 30)\n",
        "    if random.random() > 0.5:\n",
        "        degrees = -degrees\n",
        "    return pil_img.rotate(degrees)\n",
        "\n",
        "\n",
        "rotate = TransformT('Rotate', _rotate_impl)\n",
        "\n",
        "\n",
        "def _posterize_impl(pil_img, level):\n",
        "    \"\"\"Applies PIL Posterize to `pil_img`.\"\"\"\n",
        "    level = int_parameter(level, 4)\n",
        "    return ImageOps.posterize(pil_img.convert('RGB'), 4 - level).convert('RGBA')\n",
        "\n",
        "\n",
        "posterize = TransformT('Posterize', _posterize_impl)\n",
        "\n",
        "\n",
        "def _shear_x_impl(pil_img, level):\n",
        "    \"\"\"Applies PIL ShearX to `pil_img`.\n",
        "\n",
        "    The ShearX operation shears the image along the horizontal axis with `level`\n",
        "    magnitude.\n",
        "\n",
        "    Args:\n",
        "      pil_img: Image in PIL object.\n",
        "      level: Strength of the operation specified as an Integer from\n",
        "        [0, `PARAMETER_MAX`].\n",
        "\n",
        "    Returns:\n",
        "      A PIL Image that has had ShearX applied to it.\n",
        "    \"\"\"\n",
        "    level = float_parameter(level, 0.3)\n",
        "    if random.random() > 0.5:\n",
        "        level = -level\n",
        "    return pil_img.transform(pil_img.size, Image.AFFINE, (1, level, 0, 0, 1, 0))\n",
        "\n",
        "\n",
        "shear_x = TransformT('ShearX', _shear_x_impl)\n",
        "\n",
        "\n",
        "def _shear_y_impl(pil_img, level):\n",
        "    \"\"\"Applies PIL ShearY to `pil_img`.\n",
        "\n",
        "    The ShearY operation shears the image along the vertical axis with `level`\n",
        "    magnitude.\n",
        "\n",
        "    Args:\n",
        "      pil_img: Image in PIL object.\n",
        "      level: Strength of the operation specified as an Integer from\n",
        "        [0, `PARAMETER_MAX`].\n",
        "\n",
        "    Returns:\n",
        "      A PIL Image that has had ShearX applied to it.\n",
        "    \"\"\"\n",
        "    level = float_parameter(level, 0.3)\n",
        "    if random.random() > 0.5:\n",
        "        level = -level\n",
        "    return pil_img.transform(pil_img.size, Image.AFFINE, (1, 0, 0, level, 1, 0))\n",
        "\n",
        "\n",
        "shear_y = TransformT('ShearY', _shear_y_impl)\n",
        "\n",
        "\n",
        "def _translate_x_impl(pil_img, level):\n",
        "    \"\"\"Applies PIL TranslateX to `pil_img`.\n",
        "\n",
        "    Translate the image in the horizontal direction by `level`\n",
        "    number of pixels.\n",
        "\n",
        "    Args:\n",
        "      pil_img: Image in PIL object.\n",
        "      level: Strength of the operation specified as an Integer from\n",
        "        [0, `PARAMETER_MAX`].\n",
        "\n",
        "    Returns:\n",
        "      A PIL Image that has had TranslateX applied to it.\n",
        "    \"\"\"\n",
        "    level = int_parameter(level, 10)\n",
        "    if random.random() > 0.5:\n",
        "        level = -level\n",
        "    return pil_img.transform(pil_img.size, Image.AFFINE, (1, 0, level, 0, 1, 0))\n",
        "\n",
        "\n",
        "translate_x = TransformT('TranslateX', _translate_x_impl)\n",
        "\n",
        "\n",
        "def _translate_y_impl(pil_img, level):\n",
        "    \"\"\"Applies PIL TranslateY to `pil_img`.\n",
        "\n",
        "    Translate the image in the vertical direction by `level`\n",
        "    number of pixels.\n",
        "\n",
        "    Args:\n",
        "      pil_img: Image in PIL object.\n",
        "      level: Strength of the operation specified as an Integer from\n",
        "        [0, `PARAMETER_MAX`].\n",
        "\n",
        "    Returns:\n",
        "      A PIL Image that has had TranslateY applied to it.\n",
        "    \"\"\"\n",
        "    level = int_parameter(level, 10)\n",
        "    if random.random() > 0.5:\n",
        "        level = -level\n",
        "    return pil_img.transform(pil_img.size, Image.AFFINE, (1, 0, 0, 0, 1, level))\n",
        "\n",
        "\n",
        "translate_y = TransformT('TranslateY', _translate_y_impl)\n",
        "\n",
        "\n",
        "def _crop_impl(pil_img, level, interpolation=Image.BILINEAR):\n",
        "    \"\"\"Applies a crop to `pil_img` with the size depending on the `level`.\"\"\"\n",
        "    cropped = pil_img.crop((level, level, pil_img.size[0] - level, pil_img.size[1] - level))\n",
        "    resized = cropped.resize(pil_img.size, interpolation)\n",
        "    return resized\n",
        "\n",
        "\n",
        "crop_bilinear = TransformT('CropBilinear', _crop_impl)\n",
        "\n",
        "\n",
        "def _solarize_impl(pil_img, level):\n",
        "    \"\"\"Applies PIL Solarize to `pil_img`.\n",
        "\n",
        "    Translate the image in the vertical direction by `level`\n",
        "    number of pixels.\n",
        "\n",
        "    Args:\n",
        "      pil_img: Image in PIL object.\n",
        "      level: Strength of the operation specified as an Integer from\n",
        "        [0, `PARAMETER_MAX`].\n",
        "\n",
        "    Returns:\n",
        "      A PIL Image that has had Solarize applied to it.\n",
        "    \"\"\"\n",
        "    level = int_parameter(level, 256)\n",
        "    return ImageOps.solarize(pil_img.convert('RGB'), 256 - level).convert('RGBA')\n",
        "\n",
        "\n",
        "solarize = TransformT('Solarize', _solarize_impl)\n",
        "\n",
        "\n",
        "def _cutout_pil_impl(pil_img, level):\n",
        "    \"\"\"Apply cutout to pil_img at the specified level.\"\"\"\n",
        "    size = int_parameter(level, 20)\n",
        "    if size <= 0:\n",
        "        return pil_img\n",
        "    img_width, img_height = pil_img.size\n",
        "    num_channels = 3\n",
        "    _, upper_coord, lower_coord = (\n",
        "        create_cutout_mask(img_height, img_width, num_channels, size))\n",
        "    pixels = pil_img.load()  # create the pixel map\n",
        "    for i in range(upper_coord[0], lower_coord[0]):  # for every col:\n",
        "        for j in range(upper_coord[1], lower_coord[1]):  # For every row\n",
        "            pixels[i, j] = (127, 127, 127, 0)  # set the colour accordingly\n",
        "    return pil_img\n",
        "\n",
        "\n",
        "cutout = TransformT('Cutout', _cutout_pil_impl)\n",
        "\n",
        "\n",
        "def _enhancer_impl(enhancer):\n",
        "    \"\"\"Sets level to be between 0.1 and 1.8 for ImageEnhance transforms of PIL.\"\"\"\n",
        "\n",
        "    def impl(pil_img, level):\n",
        "        v = float_parameter(level, 1.8) + .1  # going to 0 just destroys it\n",
        "        return enhancer(pil_img).enhance(v)\n",
        "\n",
        "    return impl\n",
        "\n",
        "\n",
        "color = TransformT('Color', _enhancer_impl(ImageEnhance.Color))\n",
        "contrast = TransformT('Contrast', _enhancer_impl(ImageEnhance.Contrast))\n",
        "brightness = TransformT('Brightness', _enhancer_impl(\n",
        "    ImageEnhance.Brightness))\n",
        "sharpness = TransformT('Sharpness', _enhancer_impl(ImageEnhance.Sharpness))\n",
        "\n",
        "ALL_TRANSFORMS = [\n",
        "    identity,\n",
        "    flip_lr,\n",
        "    flip_ud,\n",
        "    auto_contrast,\n",
        "    equalize,\n",
        "    invert,\n",
        "    rotate,\n",
        "    posterize,\n",
        "    crop_bilinear,\n",
        "    solarize,\n",
        "    color,\n",
        "    contrast,\n",
        "    brightness,\n",
        "    sharpness,\n",
        "    shear_x,\n",
        "    shear_y,\n",
        "    translate_x,\n",
        "    translate_y,\n",
        "    cutout,\n",
        "    blur,\n",
        "    smooth\n",
        "]\n",
        "\n",
        "NAME_TO_TRANSFORM = {t.name: t for t in ALL_TRANSFORMS}\n",
        "TRANSFORM_NAMES = NAME_TO_TRANSFORM.keys()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkJt5je_wpGl"
      },
      "source": [
        "# third_party\\auto_augment\\policies.py\n",
        "# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "def cifar10_policies():\n",
        "    \"\"\"AutoAugment policies found on CIFAR-10.\"\"\"\n",
        "    exp0_0 = [[('Invert', 0.1, 7), ('Contrast', 0.2, 6)],\n",
        "              [('Rotate', 0.7, 2), ('TranslateX', 0.3, 9)],\n",
        "              [('Sharpness', 0.8, 1), ('Sharpness', 0.9, 3)],\n",
        "              [('ShearY', 0.5, 8), ('TranslateY', 0.7, 9)],\n",
        "              [('AutoContrast', 0.5, 8), ('Equalize', 0.9, 2)]]\n",
        "    exp0_1 = [[('Solarize', 0.4, 5), ('AutoContrast', 0.9, 3)],\n",
        "              [('TranslateY', 0.9, 9), ('TranslateY', 0.7, 9)],\n",
        "              [('AutoContrast', 0.9, 2), ('Solarize', 0.8, 3)],\n",
        "              [('Equalize', 0.8, 8), ('Invert', 0.1, 3)],\n",
        "              [('TranslateY', 0.7, 9), ('AutoContrast', 0.9, 1)]]\n",
        "    exp0_2 = [[('Solarize', 0.4, 5), ('AutoContrast', 0.0, 2)],\n",
        "              [('TranslateY', 0.7, 9), ('TranslateY', 0.7, 9)],\n",
        "              [('AutoContrast', 0.9, 0), ('Solarize', 0.4, 3)],\n",
        "              [('Equalize', 0.7, 5), ('Invert', 0.1, 3)],\n",
        "              [('TranslateY', 0.7, 9), ('TranslateY', 0.7, 9)]]\n",
        "    exp0_3 = [[('Solarize', 0.4, 5), ('AutoContrast', 0.9, 1)],\n",
        "              [('TranslateY', 0.8, 9), ('TranslateY', 0.9, 9)],\n",
        "              [('AutoContrast', 0.8, 0), ('TranslateY', 0.7, 9)],\n",
        "              [('TranslateY', 0.2, 7), ('Color', 0.9, 6)],\n",
        "              [('Equalize', 0.7, 6), ('Color', 0.4, 9)]]\n",
        "    exp1_0 = [[('ShearY', 0.2, 7), ('Posterize', 0.3, 7)],\n",
        "              [('Color', 0.4, 3), ('Brightness', 0.6, 7)],\n",
        "              [('Sharpness', 0.3, 9), ('Brightness', 0.7, 9)],\n",
        "              [('Equalize', 0.6, 5), ('Equalize', 0.5, 1)],\n",
        "              [('Contrast', 0.6, 7), ('Sharpness', 0.6, 5)]]\n",
        "    exp1_1 = [[('Brightness', 0.3, 7), ('AutoContrast', 0.5, 8)],\n",
        "              [('AutoContrast', 0.9, 4), ('AutoContrast', 0.5, 6)],\n",
        "              [('Solarize', 0.3, 5), ('Equalize', 0.6, 5)],\n",
        "              [('TranslateY', 0.2, 4), ('Sharpness', 0.3, 3)],\n",
        "              [('Brightness', 0.0, 8), ('Color', 0.8, 8)]]\n",
        "    exp1_2 = [[('Solarize', 0.2, 6), ('Color', 0.8, 6)],\n",
        "              [('Solarize', 0.2, 6), ('AutoContrast', 0.8, 1)],\n",
        "              [('Solarize', 0.4, 1), ('Equalize', 0.6, 5)],\n",
        "              [('Brightness', 0.0, 0), ('Solarize', 0.5, 2)],\n",
        "              [('AutoContrast', 0.9, 5), ('Brightness', 0.5, 3)]]\n",
        "    exp1_3 = [[('Contrast', 0.7, 5), ('Brightness', 0.0, 2)],\n",
        "              [('Solarize', 0.2, 8), ('Solarize', 0.1, 5)],\n",
        "              [('Contrast', 0.5, 1), ('TranslateY', 0.2, 9)],\n",
        "              [('AutoContrast', 0.6, 5), ('TranslateY', 0.0, 9)],\n",
        "              [('AutoContrast', 0.9, 4), ('Equalize', 0.8, 4)]]\n",
        "    exp1_4 = [[('Brightness', 0.0, 7), ('Equalize', 0.4, 7)],\n",
        "              [('Solarize', 0.2, 5), ('Equalize', 0.7, 5)],\n",
        "              [('Equalize', 0.6, 8), ('Color', 0.6, 2)],\n",
        "              [('Color', 0.3, 7), ('Color', 0.2, 4)],\n",
        "              [('AutoContrast', 0.5, 2), ('Solarize', 0.7, 2)]]\n",
        "    exp1_5 = [[('AutoContrast', 0.2, 0), ('Equalize', 0.1, 0)],\n",
        "              [('ShearY', 0.6, 5), ('Equalize', 0.6, 5)],\n",
        "              [('Brightness', 0.9, 3), ('AutoContrast', 0.4, 1)],\n",
        "              [('Equalize', 0.8, 8), ('Equalize', 0.7, 7)],\n",
        "              [('Equalize', 0.7, 7), ('Solarize', 0.5, 0)]]\n",
        "    exp1_6 = [[('Equalize', 0.8, 4), ('TranslateY', 0.8, 9)],\n",
        "              [('TranslateY', 0.8, 9), ('TranslateY', 0.6, 9)],\n",
        "              [('TranslateY', 0.9, 0), ('TranslateY', 0.5, 9)],\n",
        "              [('AutoContrast', 0.5, 3), ('Solarize', 0.3, 4)],\n",
        "              [('Solarize', 0.5, 3), ('Equalize', 0.4, 4)]]\n",
        "    exp2_0 = [[('Color', 0.7, 7), ('TranslateX', 0.5, 8)],\n",
        "              [('Equalize', 0.3, 7), ('AutoContrast', 0.4, 8)],\n",
        "              [('TranslateY', 0.4, 3), ('Sharpness', 0.2, 6)],\n",
        "              [('Brightness', 0.9, 6), ('Color', 0.2, 8)],\n",
        "              [('Solarize', 0.5, 2), ('Invert', 0.0, 3)]]\n",
        "    exp2_1 = [[('AutoContrast', 0.1, 5), ('Brightness', 0.0, 0)],\n",
        "              [('Cutout', 0.2, 4), ('Equalize', 0.1, 1)],\n",
        "              [('Equalize', 0.7, 7), ('AutoContrast', 0.6, 4)],\n",
        "              [('Color', 0.1, 8), ('ShearY', 0.2, 3)],\n",
        "              [('ShearY', 0.4, 2), ('Rotate', 0.7, 0)]]\n",
        "    exp2_2 = [[('ShearY', 0.1, 3), ('AutoContrast', 0.9, 5)],\n",
        "              [('TranslateY', 0.3, 6), ('Cutout', 0.3, 3)],\n",
        "              [('Equalize', 0.5, 0), ('Solarize', 0.6, 6)],\n",
        "              [('AutoContrast', 0.3, 5), ('Rotate', 0.2, 7)],\n",
        "              [('Equalize', 0.8, 2), ('Invert', 0.4, 0)]]\n",
        "    exp2_3 = [[('Equalize', 0.9, 5), ('Color', 0.7, 0)],\n",
        "              [('Equalize', 0.1, 1), ('ShearY', 0.1, 3)],\n",
        "              [('AutoContrast', 0.7, 3), ('Equalize', 0.7, 0)],\n",
        "              [('Brightness', 0.5, 1), ('Contrast', 0.1, 7)],\n",
        "              [('Contrast', 0.1, 4), ('Solarize', 0.6, 5)]]\n",
        "    exp2_4 = [[('Solarize', 0.2, 3), ('ShearX', 0.0, 0)],\n",
        "              [('TranslateX', 0.3, 0), ('TranslateX', 0.6, 0)],\n",
        "              [('Equalize', 0.5, 9), ('TranslateY', 0.6, 7)],\n",
        "              [('ShearX', 0.1, 0), ('Sharpness', 0.5, 1)],\n",
        "              [('Equalize', 0.8, 6), ('Invert', 0.3, 6)]]\n",
        "    exp2_5 = [[('AutoContrast', 0.3, 9), ('Cutout', 0.5, 3)],\n",
        "              [('ShearX', 0.4, 4), ('AutoContrast', 0.9, 2)],\n",
        "              [('ShearX', 0.0, 3), ('Posterize', 0.0, 3)],\n",
        "              [('Solarize', 0.4, 3), ('Color', 0.2, 4)],\n",
        "              [('Equalize', 0.1, 4), ('Equalize', 0.7, 6)]]\n",
        "    exp2_6 = [[('Equalize', 0.3, 8), ('AutoContrast', 0.4, 3)],\n",
        "              [('Solarize', 0.6, 4), ('AutoContrast', 0.7, 6)],\n",
        "              [('AutoContrast', 0.2, 9), ('Brightness', 0.4, 8)],\n",
        "              [('Equalize', 0.1, 0), ('Equalize', 0.0, 6)],\n",
        "              [('Equalize', 0.8, 4), ('Equalize', 0.0, 4)]]\n",
        "    exp2_7 = [[('Equalize', 0.5, 5), ('AutoContrast', 0.1, 2)],\n",
        "              [('Solarize', 0.5, 5), ('AutoContrast', 0.9, 5)],\n",
        "              [('AutoContrast', 0.6, 1), ('AutoContrast', 0.7, 8)],\n",
        "              [('Equalize', 0.2, 0), ('AutoContrast', 0.1, 2)],\n",
        "              [('Equalize', 0.6, 9), ('Equalize', 0.4, 4)]]\n",
        "    exp0s = exp0_0 + exp0_1 + exp0_2 + exp0_3\n",
        "    exp1s = exp1_0 + exp1_1 + exp1_2 + exp1_3 + exp1_4 + exp1_5 + exp1_6\n",
        "    exp2s = exp2_0 + exp2_1 + exp2_2 + exp2_3 + exp2_4 + exp2_5 + exp2_6 + exp2_7\n",
        "    return exp0s + exp1s + exp2s\n",
        "\n",
        "\n",
        "def svhn_policies():\n",
        "    \"\"\"AutoAugment policies found on SVHN.\"\"\"\n",
        "    policies = [\n",
        "        [('ShearX', 0.9, 4), ('Invert', 0.2, 3)],\n",
        "        [('ShearY', 0.9, 8), ('Invert', 0.7, 5)],\n",
        "        [('Equalize', 0.6, 5), ('Solarize', 0.6, 6)],\n",
        "        [('Invert', 0.9, 3), ('Equalize', 0.6, 3)],\n",
        "        [('Equalize', 0.6, 1), ('Rotate', 0.9, 3)],\n",
        "        [('ShearX', 0.9, 4), ('AutoContrast', 0.8, 3)],\n",
        "        [('ShearY', 0.9, 8), ('Invert', 0.4, 5)],\n",
        "        [('ShearY', 0.9, 5), ('Solarize', 0.2, 6)],\n",
        "        [('Invert', 0.9, 6), ('AutoContrast', 0.8, 1)],\n",
        "        [('Equalize', 0.6, 3), ('Rotate', 0.9, 3)],\n",
        "        [('ShearX', 0.9, 4), ('Solarize', 0.3, 3)],\n",
        "        [('ShearY', 0.8, 8), ('Invert', 0.7, 4)],\n",
        "        [('Equalize', 0.9, 5), ('TranslateY', 0.6, 6)],\n",
        "        [('Invert', 0.9, 4), ('Equalize', 0.6, 7)],\n",
        "        [('Contrast', 0.3, 3), ('Rotate', 0.8, 4)],\n",
        "        [('ShearX', 0.9, 3), ('Invert', 0.5, 3)],\n",
        "        [('ShearY', 0.9, 8), ('Invert', 0.4, 5)],\n",
        "        [('Equalize', 0.6, 3), ('Solarize', 0.2, 3)],\n",
        "        [('Invert', 0.9, 4), ('Equalize', 0.5, 6)],\n",
        "        [('Equalize', 0.6, 1), ('Rotate', 0.9, 3)],\n",
        "        [('Invert', 0.8, 5), ('TranslateY', 0.0, 2)],\n",
        "        [('ShearY', 0.7, 6), ('Solarize', 0.4, 8)],\n",
        "        [('Invert', 0.6, 4), ('Rotate', 0.8, 4)],\n",
        "        [('ShearY', 0.3, 7), ('TranslateX', 0.9, 3)],\n",
        "        [('ShearX', 0.1, 6), ('Invert', 0.6, 5)],\n",
        "        [('Solarize', 0.7, 2), ('TranslateY', 0.6, 7)],\n",
        "        [('ShearY', 0.8, 4), ('Invert', 0.8, 8)],\n",
        "        [('ShearX', 0.7, 9), ('TranslateY', 0.8, 3)],\n",
        "        [('ShearY', 0.8, 5), ('AutoContrast', 0.7, 3)],\n",
        "        [('ShearX', 0.7, 2), ('Invert', 0.1, 5)],\n",
        "        [('ShearY', 0.8, 9), ('ShearX', 0.7, 7)],\n",
        "        [('ShearY', 0.7, 4), ('Solarize', 0.9, 7)],\n",
        "        [('ShearY', 0.9, 5), ('Invert', 0.0, 4)],\n",
        "        [('TranslateX', 0.8, 3), ('ShearY', 0.7, 7)],\n",
        "        [('Invert', 0.1, 7), ('Solarize', 0.3, 9)],\n",
        "        [('Invert', 0.6, 2), ('Invert', 0.9, 4)],\n",
        "        [('Equalize', 0.5, 2), ('Solarize', 0.9, 7)],\n",
        "        [('ShearY', 0.6, 7), ('Solarize', 0.8, 3)],\n",
        "        [('ShearY', 0.6, 3), ('Invert', 0.6, 1)],\n",
        "        [('ShearX', 0.4, 2), ('Rotate', 0.7, 5)]]\n",
        "    return policies\n",
        "\n",
        "\n",
        "def imagenet_policies():\n",
        "    \"\"\"AutoAugment policies found on ImageNet.\n",
        "    This policy also transfers to five FGVC datasets with image size similar to\n",
        "    ImageNet including Oxford 102 Flowers, Caltech-101, Oxford-IIIT Pets,\n",
        "    FGVC Aircraft and Stanford Cars.\n",
        "    \"\"\"\n",
        "    policies = [\n",
        "        [('Posterize', 0.4, 8), ('Rotate', 0.6, 9)],\n",
        "        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n",
        "        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n",
        "        [('Posterize', 0.6, 7), ('Posterize', 0.6, 6)],\n",
        "        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n",
        "        [('Equalize', 0.4, 4), ('Rotate', 0.8, 8)],\n",
        "        [('Solarize', 0.6, 3), ('Equalize', 0.6, 7)],\n",
        "        [('Posterize', 0.8, 5), ('Equalize', 1.0, 2)],\n",
        "        [('Rotate', 0.2, 3), ('Solarize', 0.6, 8)],\n",
        "        [('Equalize', 0.6, 8), ('Posterize', 0.4, 6)],\n",
        "        [('Rotate', 0.8, 8), ('Color', 0.4, 0)],\n",
        "        [('Rotate', 0.4, 9), ('Equalize', 0.6, 2)],\n",
        "        [('Equalize', 0.0, 7), ('Equalize', 0.8, 8)],\n",
        "        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n",
        "        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n",
        "        [('Rotate', 0.8, 8), ('Color', 1.0, 2)],\n",
        "        [('Color', 0.8, 8), ('Solarize', 0.8, 7)],\n",
        "        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n",
        "        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n",
        "        [('Color', 0.4, 0), ('Equalize', 0.6, 3)]\n",
        "    ]\n",
        "    return policies\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWKr7SxugzZt"
      },
      "source": [
        "# libml\\augment.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Augmentations for images.\n",
        "\"\"\"\n",
        "import collections\n",
        "import functools\n",
        "import itertools\n",
        "import multiprocessing\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from absl import flags\n",
        "\n",
        "# from libml import utils, ctaugment\n",
        "# from libml.utils import EasyDict\n",
        "# from third_party.auto_augment import augmentations, policies\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "POOL = None\n",
        "POLICIES = EasyDict(cifar10=cifar10_policies(),\n",
        "                    cifar100=cifar10_policies(),\n",
        "                    svhn=svhn_policies(),\n",
        "                    svhn_noextra=svhn_policies())\n",
        "\n",
        "RANDOM_POLICY_OPS = (\n",
        "    'Identity', 'AutoContrast', 'Equalize', 'Rotate',\n",
        "    'Solarize', 'Color', 'Contrast', 'Brightness',\n",
        "    'Sharpness', 'ShearX', 'TranslateX', 'TranslateY',\n",
        "    'Posterize', 'ShearY'\n",
        ")\n",
        "AUGMENT_ENUM = 'd x m aa aac ra rac'.split() + ['r%d_%d_%d' % (nops, mag, cutout) for nops, mag, cutout in\n",
        "                                                itertools.product(range(1, 5), range(1, 16), range(0, 100, 25))] + [\n",
        "                   'rac%d' % (mag) for mag in range(1, 10)]\n",
        "\n",
        "for name in list(flags.FLAGS):\n",
        "    if name == 'K' or name == 'augment':\n",
        "      delattr(flags.FLAGS,name) \n",
        "\n",
        "flags.DEFINE_integer('K', 1, 'Number of strong augmentation for unlabeled data.')\n",
        "flags.DEFINE_enum('augment', 'd.d',\n",
        "                  [x + '.' + y for x, y in itertools.product(AUGMENT_ENUM, AUGMENT_ENUM)] +\n",
        "                  [x + '.' + y + '.' + z for x, y, z in itertools.product(AUGMENT_ENUM, AUGMENT_ENUM, AUGMENT_ENUM)] + [\n",
        "                      'd.d.d.d', 'd.aac.d.aac', 'd.rac.d.rac'],\n",
        "                  'Dataset augmentation method (x=identity, m=mirror, d=default, aa=auto-augment, aac=auto-augment+cutout, '\n",
        "                  'ra=rand-augment, rac=rand-augment+cutout; for rand-augment, magnitude is also randomized'\n",
        "                  'rxyy=random augment with x ops and magnitude yy),'\n",
        "                  'first is for labeled data, others are for unlabeled.')\n",
        "\n",
        "\n",
        "def init_pool():\n",
        "    global POOL\n",
        "    if POOL is None:\n",
        "        para = max(1, len(get_available_gpus())) * FLAGS.para_augment\n",
        "        POOL = multiprocessing.Pool(para)\n",
        "\n",
        "\n",
        "def augment_mirror(x):\n",
        "    return tf.image.random_flip_left_right(x)\n",
        "\n",
        "\n",
        "def augment_shift(x, w):\n",
        "    y = tf.pad(x, [[w] * 2, [w] * 2, [0] * 2], mode='REFLECT')\n",
        "    return tf.random_crop(y, tf.shape(x))\n",
        "\n",
        "\n",
        "def augment_noise(x, std):\n",
        "    return x + std * tf.random_normal(tf.shape(x), dtype=x.dtype)\n",
        "\n",
        "\n",
        "def numpy_apply_policy(x, policy):\n",
        "    return augmentations.apply_policy(policy, x).astype('f')\n",
        "\n",
        "\n",
        "def stack_augment(augment: list):\n",
        "    def func(x):\n",
        "        xl = [augment[i](x) if augment[i] is not None else x for i in range(len(augment))]\n",
        "        return {k: tf.stack([x[k] for x in xl]) for k in xl[0].keys()}\n",
        "\n",
        "    return func\n",
        "\n",
        "\n",
        "class Primitives:\n",
        "    @staticmethod\n",
        "    def m():\n",
        "        return lambda x: augment_mirror(x['image'])\n",
        "\n",
        "    @staticmethod\n",
        "    def ms(shift):\n",
        "        return lambda x: augment_shift(augment_mirror(x['image']), shift)\n",
        "\n",
        "    @staticmethod\n",
        "    def s(shift):\n",
        "        return lambda x: augment_shift(x['image'], shift)\n",
        "\n",
        "\n",
        "AugmentPair = collections.namedtuple('AugmentPair', 'tf numpy')\n",
        "PoolEntry = collections.namedtuple('PoolEntry', 'payload batch')\n",
        "\n",
        "\n",
        "class AugmentPool:\n",
        "    def __init__(self, get_samples):\n",
        "        self.get_samples = get_samples\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.get_samples()\n",
        "\n",
        "\n",
        "NOAUGMENT = AugmentPair(tf=lambda x: dict(image=x['image'], label=x['label'], index=x.get('index', -1)),\n",
        "                        numpy=AugmentPool)\n",
        "\n",
        "\n",
        "class AugmentPoolAA(AugmentPool):\n",
        "\n",
        "    def __init__(self, get_samples, policy_group):\n",
        "        init_pool()\n",
        "        self.get_samples = get_samples\n",
        "        self.policy_group = policy_group\n",
        "        self.queue = []\n",
        "        self.fill_queue()\n",
        "\n",
        "    @staticmethod\n",
        "    def numpy_apply_policies(arglist):\n",
        "        x, policies = arglist\n",
        "        return np.stack([augmentations.apply_policy(policy, y) for y, policy in zip(x, policies)]).astype('f')\n",
        "\n",
        "    def queue_images(self, batch):\n",
        "        args = []\n",
        "        image = batch['image']\n",
        "        if image.ndim == 4:\n",
        "            for x in range(image.shape[0]):\n",
        "                args.append((image[x:x + 1], [random.choice(POLICIES[self.policy_group])]))\n",
        "        else:\n",
        "            for x in image[:, 1:]:\n",
        "                args.append((x, [random.choice(POLICIES[self.policy_group]) for _ in range(x.shape[0])]))\n",
        "        self.queue.append(PoolEntry(payload=POOL.imap(self.numpy_apply_policies, args), batch=batch))\n",
        "\n",
        "    def fill_queue(self):\n",
        "        for _ in range(4):\n",
        "            self.queue_images(self.get_samples())\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        del args, kwargs\n",
        "        batch = self.get_samples()\n",
        "        entry = self.queue.pop(0)\n",
        "        samples = np.stack(list(entry.payload))\n",
        "        if entry.batch['image'].ndim == 4:\n",
        "            samples = samples.reshape(entry.batch['image'].shape)\n",
        "            entry.batch['image'] = samples\n",
        "        else:\n",
        "            samples = samples.reshape(entry.batch['image'][:, 1:].shape)\n",
        "            entry.batch['image'][:, 1:] = samples\n",
        "        self.queue_images(batch)\n",
        "        return entry.batch\n",
        "\n",
        "\n",
        "class AugmentPoolAAC(AugmentPoolAA):\n",
        "\n",
        "    def __init__(self, get_samples, policy_group):\n",
        "        init_pool()\n",
        "        self.get_samples = get_samples\n",
        "        self.policy_group = policy_group\n",
        "        self.queue = []\n",
        "        self.fill_queue()\n",
        "\n",
        "    @staticmethod\n",
        "    def numpy_apply_policies(arglist):\n",
        "        x, policies = arglist\n",
        "        return np.stack([augmentations.cutout_numpy(augmentations.apply_policy(policy, y)) for y, policy in\n",
        "                         zip(x, policies)]).astype('f')\n",
        "\n",
        "\n",
        "class AugmentPoolRAM(AugmentPoolAA):\n",
        "    # Randomized magnitude\n",
        "    def __init__(self, get_samples, nops=2, magnitude=10):\n",
        "        init_pool()\n",
        "        self.get_samples = get_samples\n",
        "        self.nops = nops\n",
        "        self.magnitude = magnitude\n",
        "        self.queue = []\n",
        "        self.fill_queue()\n",
        "\n",
        "    @staticmethod\n",
        "    def numpy_apply_policies(arglist):\n",
        "        x, policies = arglist\n",
        "        return np.stack([augmentations.apply_policy(policy, y) for y, policy in zip(x, policies)]).astype('f')\n",
        "\n",
        "    def queue_images(self, batch):\n",
        "        args = []\n",
        "        image = batch['image']\n",
        "        policy = lambda: [(op, 0.5, np.random.randint(1, self.magnitude))\n",
        "                          for op in np.random.choice(RANDOM_POLICY_OPS, self.nops)]\n",
        "        if image.ndim == 4:\n",
        "            for x in range(image.shape[0]):\n",
        "                args.append((image[x:x + 1], [policy()]))\n",
        "        else:\n",
        "            for x in image[:, 1:]:\n",
        "                args.append((x, [policy() for _ in range(x.shape[0])]))\n",
        "        self.queue.append(PoolEntry(payload=POOL.imap(self.numpy_apply_policies, args), batch=batch))\n",
        "\n",
        "\n",
        "class AugmentPoolRAMC(AugmentPoolRAM):\n",
        "    # Randomized magnitude (inherited from queue images)\n",
        "    def __init__(self, get_samples, nops=2, magnitude=10):\n",
        "        init_pool()\n",
        "        self.get_samples = get_samples\n",
        "        self.nops = nops\n",
        "        self.magnitude = magnitude\n",
        "        self.queue = []\n",
        "        self.fill_queue()\n",
        "\n",
        "    @staticmethod\n",
        "    def numpy_apply_policies(arglist):\n",
        "        x, policies = arglist\n",
        "        return np.stack([augmentations.cutout_numpy(augmentations.apply_policy(policy, y)) for y, policy in\n",
        "                         zip(x, policies)]).astype('f')\n",
        "\n",
        "\n",
        "class AugmentPoolRAMC2(AugmentPoolRAM):\n",
        "    # Randomized magnitude (inherited from queue images)\n",
        "    def __init__(self, get_samples, nops=2, magnitude=10):\n",
        "        init_pool()\n",
        "        self.get_samples = get_samples\n",
        "        self.nops = nops\n",
        "        self.magnitude = magnitude\n",
        "        self.queue = []\n",
        "        self.fill_queue()\n",
        "\n",
        "    @staticmethod\n",
        "    def numpy_apply_policies(arglist):\n",
        "        x, policies = arglist\n",
        "        return np.stack([augmentations.cutout_numpy(augmentations.apply_policy(policy, y)) for y, policy in\n",
        "                         zip(x, policies)]).astype('f')\n",
        "\n",
        "    def queue_images(self, batch):\n",
        "        args = []\n",
        "        image = batch['image']\n",
        "        policy = lambda: [(op, 0.5, np.random.randint(1, self.magnitude))\n",
        "                          for op in np.random.choice(RANDOM_POLICY_OPS, self.nops)]\n",
        "        if image.ndim == 4:\n",
        "            for x in range(image.shape[0]):\n",
        "                args.append((image[x:x + 1], [policy()]))\n",
        "        else:\n",
        "            for x in image[:, :]:\n",
        "                args.append((x, [policy() for _ in range(x.shape[0])]))\n",
        "        self.queue.append(PoolEntry(payload=POOL.imap(self.numpy_apply_policies, args), batch=batch))\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        del args, kwargs\n",
        "        batch = self.get_samples()\n",
        "        entry = self.queue.pop(0)\n",
        "        samples = np.stack(list(entry.payload))\n",
        "        if entry.batch['image'].ndim == 4:\n",
        "            samples = samples.reshape(entry.batch['image'].shape)\n",
        "            entry.batch['image'] = samples\n",
        "        else:\n",
        "            samples = samples.reshape(entry.batch['image'][:, :].shape)\n",
        "            entry.batch['image'][:, :] = samples\n",
        "        self.queue_images(batch)\n",
        "        return entry.batch\n",
        "\n",
        "\n",
        "class AugmentPoolRA(AugmentPoolAA):\n",
        "    def __init__(self, get_samples, nops, magnitude, cutout):\n",
        "        init_pool()\n",
        "        self.get_samples = get_samples\n",
        "        self.nops = nops\n",
        "        self.magnitude = magnitude\n",
        "        self.size = cutout\n",
        "        self.queue = []\n",
        "        self.fill_queue()\n",
        "\n",
        "    @staticmethod\n",
        "    def numpy_apply_policies(arglist):\n",
        "        x, policies, cutout = arglist\n",
        "        return np.stack([augmentations.cutout_numpy(augmentations.apply_policy(policy, y),\n",
        "                                                    size=int(0.01 * cutout * min(y.shape[:2])))\n",
        "                         for y, policy in zip(x, policies)]).astype('f')\n",
        "\n",
        "    def queue_images(self, batch):\n",
        "        args = []\n",
        "        image = batch['image']\n",
        "        # Fixed magnitude\n",
        "        policy = lambda: [(op, 1.0, self.magnitude) for op in np.random.choice(RANDOM_POLICY_OPS, self.nops)]\n",
        "        if image.ndim == 4:\n",
        "            for x in range(image.shape[0]):\n",
        "                args.append((image[x:x + 1], [policy()], self.size))\n",
        "        else:\n",
        "            for x in image[:, 1:]:\n",
        "                args.append((x, [policy() for _ in range(x.shape[0])], self.size))\n",
        "        self.queue.append(PoolEntry(payload=POOL.imap(self.numpy_apply_policies, args), batch=batch))\n",
        "\n",
        "\n",
        "class AugmentPoolCTA(AugmentPool):\n",
        "\n",
        "    def __init__(self, get_samples):\n",
        "        init_pool()\n",
        "        self.get_samples = get_samples\n",
        "        self.queue = []\n",
        "        self.fill_queue()\n",
        "\n",
        "    @staticmethod\n",
        "    def numpy_apply_policies(arglist):\n",
        "        x, cta, probe = arglist\n",
        "        if x.ndim == 3:\n",
        "            assert probe\n",
        "            policy = cta.policy(probe=True)\n",
        "            return dict(policy=policy,\n",
        "                        probe=ctaugment.apply(x, policy),\n",
        "                        image=ctaugment.apply(x, cta.policy(probe=False)))\n",
        "        assert not probe\n",
        "        return dict(image=np.stack([x[0]] + [ctaugment.apply(y, cta.policy(probe=False)) for y in x[1:]]).astype('f'))\n",
        "\n",
        "    def queue_images(self):\n",
        "        batch = self.get_samples()\n",
        "        args = [(x, batch['cta'], batch['probe']) for x in batch['image']]\n",
        "        self.queue.append(PoolEntry(payload=POOL.imap(self.numpy_apply_policies, args), batch=batch))\n",
        "\n",
        "    def fill_queue(self):\n",
        "        for _ in range(4):\n",
        "            self.queue_images()\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        del args, kwargs\n",
        "        entry = self.queue.pop(0)\n",
        "        samples = list(entry.payload)\n",
        "        entry.batch['image'] = np.stack(x['image'] for x in samples)\n",
        "        if 'probe' in samples[0]:\n",
        "            entry.batch['probe'] = np.stack(x['probe'] for x in samples)\n",
        "            entry.batch['policy'] = [x['policy'] for x in samples]\n",
        "        self.queue_images()\n",
        "        return entry.batch\n",
        "\n",
        "\n",
        "DEFAULT_AUGMENT = EasyDict(\n",
        "    cifar10=AugmentPair(tf=lambda x: dict(image=Primitives.ms(4)(x), label=x['label'], index=x.get('index', -1)),\n",
        "                        numpy=AugmentPool),\n",
        "    cifar100=AugmentPair(tf=lambda x: dict(image=Primitives.ms(4)(x), label=x['label'], index=x.get('index', -1)),\n",
        "                         numpy=AugmentPool),\n",
        "    fashion_mnist=AugmentPair(tf=lambda x: dict(image=Primitives.ms(4)(x), label=x['label'], index=x.get('index', -1)),\n",
        "                              numpy=AugmentPool),\n",
        "    stl10=AugmentPair(tf=lambda x: dict(image=Primitives.ms(12)(x), label=x['label'], index=x.get('index', -1)),\n",
        "                      numpy=AugmentPool),\n",
        "    svhn=AugmentPair(tf=lambda x: dict(image=Primitives.s(4)(x), label=x['label'], index=x.get('index', -1)),\n",
        "                     numpy=AugmentPool),\n",
        "    svhn_noextra=AugmentPair(tf=lambda x: dict(image=Primitives.s(4)(x), label=x['label'], index=x.get('index', -1)),\n",
        "                             numpy=AugmentPool),\n",
        ")\n",
        "AUTO_AUGMENT = EasyDict({\n",
        "    k: AugmentPair(tf=v.tf, numpy=functools.partial(AugmentPoolAA, policy_group=k))\n",
        "    for k, v in DEFAULT_AUGMENT.items()\n",
        "})\n",
        "AUTO_AUGMENT_CUTOUT = EasyDict({\n",
        "    k: AugmentPair(tf=v.tf, numpy=functools.partial(AugmentPoolAAC, policy_group=k))\n",
        "    for k, v in DEFAULT_AUGMENT.items()\n",
        "})\n",
        "RAND_AUGMENT = EasyDict({\n",
        "    k: AugmentPair(tf=v.tf, numpy=functools.partial(AugmentPoolRAM, nops=2, magnitude=10))\n",
        "    for k, v in DEFAULT_AUGMENT.items()\n",
        "})\n",
        "RAND_AUGMENT_CUTOUT = EasyDict({\n",
        "    k: AugmentPair(tf=v.tf, numpy=functools.partial(AugmentPoolRAMC, nops=2, magnitude=10))\n",
        "    for k, v in DEFAULT_AUGMENT.items()\n",
        "})\n",
        "\n",
        "\n",
        "def get_augmentation(dataset: str, augmentation: str):\n",
        "    if augmentation == 'x':\n",
        "        return NOAUGMENT\n",
        "    elif augmentation == 'm':\n",
        "        return AugmentPair(tf=lambda x: dict(image=Primitives.m()(x), label=x['label'], index=x.get('index', -1)),\n",
        "                           numpy=AugmentPool)\n",
        "    elif augmentation == 'd':\n",
        "        return DEFAULT_AUGMENT[dataset]\n",
        "    elif augmentation == 'aa':\n",
        "        return AUTO_AUGMENT[dataset]\n",
        "    elif augmentation == 'aac':\n",
        "        return AUTO_AUGMENT_CUTOUT[dataset]\n",
        "    elif augmentation == 'ra':\n",
        "        return RAND_AUGMENT[dataset]\n",
        "    elif augmentation.startswith('rac'):\n",
        "        mag = 10 if augmentation == 'rac' else int(augmentation[-1])\n",
        "        return AugmentPair(tf=DEFAULT_AUGMENT[dataset].tf,\n",
        "                           numpy=functools.partial(AugmentPoolRAMC, nops=2, magnitude=mag))\n",
        "    elif augmentation[0] == 'r':\n",
        "        nops, mag, cutout = (int(x) for x in augmentation[1:].split('_'))\n",
        "        return AugmentPair(tf=DEFAULT_AUGMENT[dataset].tf,\n",
        "                           numpy=functools.partial(AugmentPoolRA, nops=nops, magnitude=mag, cutout=cutout))\n",
        "    else:\n",
        "        raise NotImplementedError(augmentation)\n",
        "\n",
        "\n",
        "def augment_function(dataset: str):\n",
        "    augmentations = FLAGS.augment.split('.')\n",
        "    assert len(augmentations) == 2\n",
        "    return [get_augmentation(dataset, x) for x in augmentations]\n",
        "\n",
        "\n",
        "def pair_augment_function(dataset: str):\n",
        "    augmentations = FLAGS.augment.split('.')\n",
        "    assert len(augmentations) == 3\n",
        "    unlabeled = [get_augmentation(dataset, x) for x in augmentations[1:]]\n",
        "    return [get_augmentation(dataset, augmentations[0]),\n",
        "            AugmentPair(tf=stack_augment([x.tf for x in unlabeled]), numpy=unlabeled[-1].numpy)]\n",
        "\n",
        "\n",
        "def quad_augment_function(dataset: str):\n",
        "    augmentations = FLAGS.augment.split('.')\n",
        "    assert len(augmentations) == 4\n",
        "    labeled = [get_augmentation(dataset, x) for x in augmentations[:2]]\n",
        "    unlabeled = [get_augmentation(dataset, x) for x in augmentations[2:]]\n",
        "    return [AugmentPair(tf=stack_augment([x.tf for x in labeled]), numpy=labeled[-1].numpy),\n",
        "            AugmentPair(tf=stack_augment([x.tf for x in unlabeled]), numpy=unlabeled[-1].numpy)]\n",
        "\n",
        "\n",
        "def many_augment_function(dataset: str):\n",
        "    augmentations = FLAGS.augment.split('.')\n",
        "    assert len(augmentations) == 3\n",
        "    unlabeled = [get_augmentation(dataset, x) for x in (augmentations[1:2] + augmentations[2:] * FLAGS.K)]\n",
        "    return [get_augmentation(dataset, augmentations[0]),\n",
        "            AugmentPair(tf=stack_augment([x.tf for x in unlabeled]), numpy=unlabeled[-1].numpy)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5lmT9jWy52g"
      },
      "source": [
        "-we used another method, described above using an index for sub dataset,\n",
        "iDataset = {0 : 19}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZDkB5tkgkRE"
      },
      "source": [
        "# libml\\data.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Input data for image models.\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# NOAM\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from libml import augment as augment_module\n",
        "# from libml import utils\n",
        "# from libml.augment import AugmentPair, NOAUGMENT\n",
        "\n",
        "# Data directory. Value is initialized in _data_setup\n",
        "#\n",
        "# Note that if you need to use DATA_DIR outside of this module then\n",
        "# you should do following:\n",
        "#     from libml import data as libml_data\n",
        "#     ...\n",
        "#     dir = libml_data.DATA_DIR\n",
        "#\n",
        "# If you directly import DATA_DIR:\n",
        "#   from libml.data import DATA_DIR\n",
        "# then None will be imported.\n",
        "DATA_DIR = None\n",
        "\n",
        "_DATA_CACHE = None\n",
        "SAMPLES_PER_CLASS = [1, 2, 3, 4, 5, 10, 25, 100, 400]\n",
        "\n",
        "for name in list(flags.FLAGS):\n",
        "    if name == 'dataset' or name == 'para_parse' or name == 'para_augment' or name == 'shuffle' or name == 'p_unlabeled' or name == 'whiten' or name == 'data_dir':\n",
        "      delattr(flags.FLAGS,name) \n",
        "\n",
        "flags.DEFINE_string('dataset', 'cifar10.1@4000-5000', 'Data to train on.')\n",
        "flags.DEFINE_integer('para_parse', 1, 'Parallel parsing.')\n",
        "flags.DEFINE_integer('para_augment', 5, 'Parallel augmentation.')\n",
        "flags.DEFINE_integer('shuffle', 8192, 'Size of dataset shuffling.')\n",
        "flags.DEFINE_string('p_unlabeled', '', 'Probability distribution of unlabeled.')\n",
        "flags.DEFINE_bool('whiten', False, 'Whether to normalize images.')\n",
        "flags.DEFINE_string('data_dir', None,\n",
        "                    'Data directory. '\n",
        "                    'If None then environment variable ML_DATA '\n",
        "                    'will be used as a data directory.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "def _data_setup():\n",
        "    # set up data directory\n",
        "    global DATA_DIR\n",
        "    # DATA_DIR = FLAGS.data_dir or os.environ['ML_DATA']    #NOAM\n",
        "    DATA_DIR = \"/content/drive/MyDrive/MachineLearning/Final_Project_Fix_Match/SAVED_DATA/\"\n",
        "\n",
        "\n",
        "app.call_after_init(_data_setup)\n",
        "\n",
        "\n",
        "def record_parse_mnist(serialized_example, image_shape=None):\n",
        "    features = tf.parse_single_example(\n",
        "        serialized_example,\n",
        "        features={'image': tf.FixedLenFeature([], tf.string),\n",
        "                  'label': tf.FixedLenFeature([], tf.int64)})\n",
        "    image = tf.image.decode_image(features['image'])\n",
        "    if image_shape:\n",
        "        image.set_shape(image_shape)\n",
        "    image = tf.pad(image, [[2] * 2, [2] * 2, [0] * 2])\n",
        "    image = tf.cast(image, tf.float32) * (2.0 / 255) - 1.0\n",
        "    return dict(image=image, label=features['label'])\n",
        "\n",
        "\n",
        "def record_parse(serialized_example, image_shape=None):\n",
        "    features = tf.parse_single_example(\n",
        "        serialized_example,\n",
        "        features={'image': tf.FixedLenFeature([], tf.string),\n",
        "                  'label': tf.FixedLenFeature([], tf.int64)})\n",
        "    image = tf.image.decode_image(features['image'])\n",
        "    if image_shape:\n",
        "        image.set_shape(image_shape)\n",
        "    image = tf.cast(image, tf.float32) * (2.0 / 255) - 1.0\n",
        "    return dict(image=image, label=features['label'])\n",
        "\n",
        "\n",
        "def compute_mean_std(data: tf.data.Dataset):\n",
        "    data = data.map(lambda x: x['image']).batch(1024).prefetch(1)\n",
        "    data = data.make_one_shot_iterator().get_next()\n",
        "    count = 0\n",
        "    stats = []\n",
        "    with tf.Session(config=get_config()) as sess:\n",
        "        def iterator():\n",
        "            while True:\n",
        "                try:\n",
        "                    yield sess.run(data)\n",
        "                except tf.errors.OutOfRangeError:\n",
        "                    break\n",
        "\n",
        "        for batch in tqdm(iterator(), unit='kimg', desc='Computing dataset mean and std'):\n",
        "            ratio = batch.shape[0] / 1024.\n",
        "            count += ratio\n",
        "            stats.append((batch.mean((0, 1, 2)) * ratio, (batch ** 2).mean((0, 1, 2)) * ratio))\n",
        "    mean = sum(x[0] for x in stats) / count\n",
        "    sigma = sum(x[1] for x in stats) / count - mean ** 2\n",
        "    std = np.sqrt(sigma)\n",
        "    print('Mean %s  Std: %s' % (mean, std))\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "class DataSet:\n",
        "    \"\"\"Wrapper for tf.data.Dataset to permit extensions.\"\"\"\n",
        "\n",
        "    def __init__(self, data: tf.data.Dataset, augment_fn: AugmentPair, parse_fn=record_parse, image_shape=None):\n",
        "        self.data = data\n",
        "        self.parse_fn = parse_fn\n",
        "        self.augment_fn = augment_fn\n",
        "        self.image_shape = image_shape\n",
        "\n",
        "    @classmethod\n",
        "    def from_files(cls, filenames: list, augment_fn: AugmentPair, parse_fn=record_parse, image_shape=None):\n",
        "        filenames_in = filenames\n",
        "        filenames = sorted(sum([tf.io.gfile.Glob(x) for x in filenames], []))\n",
        "        if not filenames:\n",
        "            raise ValueError('Empty dataset, did you mount gcsfuse bucket?', filenames_in)\n",
        "        if len(filenames) > 4:\n",
        "            def fetch_dataset(filename):\n",
        "                buffer_size = 8 * 1024 * 1024  # 8 MiB per file\n",
        "                dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n",
        "                return dataset\n",
        "\n",
        "            # Read the data from disk in parallel\n",
        "            dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "            dataset = dataset.apply(\n",
        "                tf.data.experimental.parallel_interleave(\n",
        "                    fetch_dataset,\n",
        "                    cycle_length=min(16, len(filenames)),\n",
        "                    sloppy=True))\n",
        "        else:\n",
        "            dataset = tf.data.TFRecordDataset(filenames)\n",
        "        return cls(dataset,\n",
        "                   augment_fn=augment_fn,\n",
        "                   parse_fn=parse_fn,\n",
        "                   image_shape=image_shape)\n",
        "\n",
        "    @classmethod\n",
        "    def empty_data(cls, image_shape, augment_fn: AugmentPair = None):\n",
        "        def _get_null_input(_):\n",
        "            return dict(image=tf.zeros(image_shape, tf.float32),\n",
        "                        label=tf.constant(0, tf.int64))\n",
        "\n",
        "        return cls(tf.data.Dataset.range(FLAGS.batch).map(_get_null_input),\n",
        "                   parse_fn=None,\n",
        "                   augment_fn=augment_fn,\n",
        "                   image_shape=image_shape)\n",
        "\n",
        "    def __getattr__(self, item):\n",
        "        if item in self.__dict__:\n",
        "            return self.__dict__[item]\n",
        "\n",
        "        def call_and_update(*args, **kwargs):\n",
        "            v = getattr(self.__dict__['data'], item)(*args, **kwargs)\n",
        "            if isinstance(v, tf.data.Dataset):\n",
        "                return self.__class__(v,\n",
        "                                      parse_fn=self.parse_fn,\n",
        "                                      augment_fn=self.augment_fn,\n",
        "                                      image_shape=self.image_shape)\n",
        "            return v\n",
        "\n",
        "        return call_and_update\n",
        "\n",
        "    def parse(self):\n",
        "        if self.parse_fn:\n",
        "            para = 4 * max(1, len(get_available_gpus())) * FLAGS.para_parse\n",
        "            if self.image_shape:\n",
        "                return self.map(lambda x: self.parse_fn(x, self.image_shape), para)\n",
        "            else:\n",
        "                return self.map(self.parse_fn, para)\n",
        "        return self\n",
        "\n",
        "    def numpy_augment(self, *args, **kwargs):\n",
        "        return self.augment_fn.numpy(*args, **kwargs)\n",
        "\n",
        "    def augment(self):\n",
        "        if self.augment_fn:\n",
        "            para = max(1, len(get_available_gpus())) * FLAGS.para_augment\n",
        "            return self.map(self.augment_fn.tf, para)\n",
        "        return self\n",
        "\n",
        "    def memoize(self):\n",
        "        \"\"\"Call before parsing, since it calls for parse inside.\"\"\"\n",
        "        data = []\n",
        "        with tf.Session(config=get_config()) as session:\n",
        "            it = self.parse().prefetch(16).make_one_shot_iterator().get_next()\n",
        "            try:\n",
        "                while 1:\n",
        "                    data.append(session.run(it))\n",
        "            except tf.errors.OutOfRangeError:\n",
        "                pass\n",
        "        images = np.stack([x['image'] for x in data])\n",
        "        labels = np.stack([x['label'] for x in data])\n",
        "\n",
        "        def tf_get(index, image_shape):\n",
        "            def get(index):\n",
        "                return images[index], labels[index]\n",
        "\n",
        "            image, label = tf.py_func(get, [index], [tf.float32, tf.int64])\n",
        "            return dict(image=tf.reshape(image, image_shape), label=label, index=index)\n",
        "\n",
        "        return self.__class__(tf.data.Dataset.range(len(data)),\n",
        "                              parse_fn=tf_get,\n",
        "                              augment_fn=self.augment_fn,\n",
        "                              image_shape=self.image_shape)\n",
        "\n",
        "\n",
        "class DataSets:\n",
        "    def __init__(self, name, train_labeled: DataSet, train_unlabeled: DataSet, test: DataSet, valid: DataSet,\n",
        "                 height=32, width=32, colors=3, nclass=10, mean=0, std=1, p_labeled=None, p_unlabeled=None):\n",
        "        self.name = name\n",
        "        self.train_labeled = train_labeled\n",
        "        self.train_unlabeled = train_unlabeled\n",
        "        self.test = test\n",
        "        self.valid = valid\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.colors = colors\n",
        "        self.nclass = nclass\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.p_labeled = p_labeled\n",
        "        self.p_unlabeled = p_unlabeled\n",
        "\n",
        "    @classmethod\n",
        "    def creator(cls, name, seed, label, valid, augment, parse_fn=record_parse, do_memoize=False,\n",
        "                nclass=10, colors=3, height=32, width=32):\n",
        "        if not isinstance(augment, list):\n",
        "            augment = augment(name)\n",
        "        fullname = '.%d@%d' % (seed, label)\n",
        "        root = os.path.join(DATA_DIR, 'SSL2', name)\n",
        "\n",
        "        def create():\n",
        "            p_labeled = p_unlabeled = None\n",
        "\n",
        "            if FLAGS.p_unlabeled:\n",
        "                sequence = FLAGS.p_unlabeled.split(',')\n",
        "                p_unlabeled = np.array(list(map(float, sequence)), dtype=np.float32)\n",
        "                p_unlabeled /= np.max(p_unlabeled)\n",
        "\n",
        "            image_shape = [height, width, colors]\n",
        "            train_labeled = DataSet.from_files(\n",
        "                [root + fullname + '-label.tfrecord'], augment[0], parse_fn, image_shape)\n",
        "            train_unlabeled = DataSet.from_files(\n",
        "                [root + '-unlabel.tfrecord'], augment[1], parse_fn, image_shape)\n",
        "            if do_memoize:\n",
        "                train_labeled = train_labeled.memoize()\n",
        "                train_unlabeled = train_unlabeled.memoize()\n",
        "\n",
        "            if FLAGS.whiten:\n",
        "                mean, std = compute_mean_std(train_labeled.concatenate(train_unlabeled))\n",
        "            else:\n",
        "                mean, std = 0, 1\n",
        "\n",
        "            test_data = DataSet.from_files(\n",
        "                [os.path.join(DATA_DIR, '%s-test.tfrecord' % name)], NOAUGMENT, parse_fn, image_shape=image_shape)\n",
        "\n",
        "            return cls(name + '.' + FLAGS.augment + fullname + '-' + str(valid)\n",
        "                       + ('/' + FLAGS.p_unlabeled if FLAGS.p_unlabeled else ''),\n",
        "                       train_labeled=train_labeled,\n",
        "                       train_unlabeled=train_unlabeled.skip(valid),\n",
        "                       valid=train_unlabeled.take(valid),\n",
        "                       test=test_data,\n",
        "                       nclass=nclass, p_labeled=p_labeled, p_unlabeled=p_unlabeled,\n",
        "                       height=height, width=width, colors=colors, mean=mean, std=std)\n",
        "\n",
        "        return name + fullname + '-' + str(valid), create\n",
        "\n",
        "\n",
        "def create_datasets(augment_fn):\n",
        "    d = {}\n",
        "    d.update([DataSets.creator('cifar10', seed, label, valid, augment_fn)\n",
        "              for seed, label, valid in itertools.product(range(6), [10 * x for x in SAMPLES_PER_CLASS], [1, 5000])])\n",
        "    d.update([DataSets.creator('cifar100', seed, label, valid, augment_fn, nclass=100)\n",
        "              for seed, label, valid in itertools.product(range(6), [400, 1000, 2500, 10000], [1, 5000])])\n",
        "\n",
        "    # NOAM\n",
        "    \n",
        "    # d.update([DataSets.creator('fashion_mnist', seed, label, valid, augment_fn, height=32, width=32, colors=1,\n",
        "    #                            parse_fn=record_parse_mnist)\n",
        "    #           for seed, label, valid in itertools.product(range(6), [10 * x for x in SAMPLES_PER_CLASS], [1, 5000])])\n",
        "    # d.update([DataSets.creator('stl10', seed, label, valid, augment_fn, height=96, width=96)\n",
        "    #           for seed, label, valid in itertools.product(range(6), [1000, 5000], [1, 500])])\n",
        "    # d.update([DataSets.creator('svhn', seed, label, valid, augment_fn)\n",
        "    #           for seed, label, valid in itertools.product(range(6), [10 * x for x in SAMPLES_PER_CLASS], [1, 5000])])\n",
        "    # d.update([DataSets.creator('svhn_noextra', seed, label, valid, augment_fn)\n",
        "    #           for seed, label, valid in itertools.product(range(6), [10 * x for x in SAMPLES_PER_CLASS], [1, 5000])])\n",
        "    return d\n",
        "\n",
        "\n",
        "DATASETS = functools.partial(create_datasets, augment_function)\n",
        "PAIR_DATASETS = functools.partial(create_datasets, pair_augment_function)\n",
        "MANY_DATASETS = functools.partial(create_datasets, many_augment_function)\n",
        "QUAD_DATASETS = functools.partial(create_datasets, quad_augment_function)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUHFdKvAgX2V"
      },
      "source": [
        "#libml\\train.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Training loop, checkpoint saving and loading, evaluation code.\"\"\"\n",
        "import functools\n",
        "import json\n",
        "import os.path\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from absl import flags\n",
        "from tqdm import trange, tqdm\n",
        "\n",
        "# from libml import data, utils\n",
        "# from libml.utils import EasyDict\n",
        "\n",
        "\n",
        "for name in list(flags.FLAGS):\n",
        "    if name == 'train_dir' or name == 'lr' or name == 'batch' or name == 'train_kimg' or name == 'report_kimg' or name == 'save_kimg' or name == 'keep_ckpt' or name == 'eval_ckpt' or name == 'rerun':\n",
        "      delattr(flags.FLAGS,name) \n",
        "\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_string('train_dir', './experiments',\n",
        "                    'Folder where to save training data.')\n",
        "flags.DEFINE_float('lr', 0.0001, 'Learning rate.')\n",
        "flags.DEFINE_integer('batch', 64, 'Batch size.')\n",
        "flags.DEFINE_integer('train_kimg', 1 << 14, 'Training duration in kibi-samples.')\n",
        "flags.DEFINE_integer('report_kimg', 64, 'Report summary period in kibi-samples.')\n",
        "flags.DEFINE_integer('save_kimg', 64, 'Save checkpoint period in kibi-samples.')\n",
        "flags.DEFINE_integer('keep_ckpt', 50, 'Number of checkpoints to keep.')\n",
        "flags.DEFINE_string('eval_ckpt', '', 'Checkpoint to evaluate. If provided, do not do training, just do eval.')\n",
        "flags.DEFINE_string('rerun', '', 'A string to identify a run if running multiple ones with same parameters.')\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, train_dir: str, dataset: DataSets, **kwargs):\n",
        "        self.train_dir = os.path.join(train_dir, FLAGS.rerun, self.experiment_name(**kwargs))\n",
        "        self.params = EasyDict(kwargs)\n",
        "        self.dataset = dataset\n",
        "        self.session = None\n",
        "        self.tmp = EasyDict(print_queue=[], cache=EasyDict())\n",
        "        self.step = tf.train.get_or_create_global_step()\n",
        "        self.ops = self.model(**kwargs)\n",
        "        self.ops.update_step = tf.assign_add(self.step, FLAGS.batch)\n",
        "        self.add_summaries(**kwargs)\n",
        "\n",
        "        print(' Config '.center(80, '-'))\n",
        "        print('train_dir', self.train_dir)\n",
        "        print('%-32s %s' % ('Model', self.__class__.__name__))\n",
        "        print('%-32s %s' % ('Dataset', dataset.name))\n",
        "        for k, v in sorted(kwargs.items()):\n",
        "            print('%-32s %s' % (k, v))\n",
        "        print(' Model '.center(80, '-'))\n",
        "        to_print = [tuple(['%s' % x for x in (v.name, np.prod(v.shape), v.shape)]) for v in model_vars(None)]\n",
        "        to_print.append(('Total', str(sum(int(x[1]) for x in to_print)), ''))\n",
        "        sizes = [max([len(x[i]) for x in to_print]) for i in range(3)]\n",
        "        fmt = '%%-%ds  %%%ds  %%%ds' % tuple(sizes)\n",
        "        for x in to_print[:-1]:\n",
        "            print(fmt % x)\n",
        "        print()\n",
        "        print(fmt % to_print[-1])\n",
        "        print('-' * 80)\n",
        "        self._create_initial_files()\n",
        "\n",
        "    @property\n",
        "    def arg_dir(self):\n",
        "        return os.path.join(self.train_dir, 'args')\n",
        "\n",
        "    @property\n",
        "    def checkpoint_dir(self):\n",
        "        return os.path.join(self.train_dir, 'tf')\n",
        "\n",
        "    def train_print(self, text):\n",
        "        self.tmp.print_queue.append(text)\n",
        "\n",
        "    def _create_initial_files(self):\n",
        "        for dir in (self.checkpoint_dir, self.arg_dir):\n",
        "            tf.gfile.MakeDirs(dir)\n",
        "        self.save_args()\n",
        "\n",
        "    def _reset_files(self):\n",
        "        shutil.rmtree(self.train_dir)\n",
        "        self._create_initial_files()\n",
        "\n",
        "    def save_args(self, **extra_params):\n",
        "        with tf.gfile.Open(os.path.join(self.arg_dir, 'args.json'), 'w') as f:\n",
        "            json.dump({**self.params, **extra_params}, f, sort_keys=True, indent=4)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, train_dir):\n",
        "        with tf.gfile.Open(os.path.join(train_dir, 'args/args.json'), 'r') as f:\n",
        "            params = json.load(f)\n",
        "        instance = cls(train_dir=train_dir, **params)\n",
        "        instance.train_dir = train_dir\n",
        "        return instance\n",
        "\n",
        "    def experiment_name(self, **kwargs):\n",
        "        args = [x + str(y) for x, y in sorted(kwargs.items())]\n",
        "        return '_'.join([self.__class__.__name__] + args)\n",
        "\n",
        "    def eval_mode(self, ckpt=None):\n",
        "        self.session = tf.Session(config=get_config())\n",
        "        saver = tf.train.Saver()\n",
        "        if ckpt is None:\n",
        "            ckpt = find_latest_checkpoint(self.checkpoint_dir)\n",
        "        else:\n",
        "            ckpt = os.path.abspath(ckpt)\n",
        "        saver.restore(self.session, ckpt)\n",
        "        self.tmp.step = self.session.run(self.step)\n",
        "        print('Eval model %s at global_step %d' % (self.__class__.__name__, self.tmp.step))\n",
        "        return self\n",
        "\n",
        "    def model(self, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def add_summaries(self, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class ClassifySemi(Model):\n",
        "    \"\"\"Semi-supervised classification.\"\"\"\n",
        "\n",
        "    def __init__(self, train_dir: str, dataset: DataSets, nclass: int, **kwargs):\n",
        "        self.nclass = nclass\n",
        "        Model.__init__(self, train_dir, dataset, nclass=nclass, **kwargs)\n",
        "\n",
        "    def train_step(self, train_session, gen_labeled, gen_unlabeled):\n",
        "        x, y = gen_labeled(), gen_unlabeled()\n",
        "        self.tmp.step = train_session.run([self.ops.train_op, self.ops.update_step],\n",
        "                                          feed_dict={self.ops.y: y['image'],\n",
        "                                                     self.ops.xt: x['image'],\n",
        "                                                     self.ops.label: x['label']})[1]\n",
        "\n",
        "    def gen_labeled_fn(self, data_iterator):\n",
        "        return self.dataset.train_labeled.numpy_augment(lambda: self.session.run(data_iterator))\n",
        "\n",
        "    def gen_unlabeled_fn(self, data_iterator):\n",
        "        return self.dataset.train_unlabeled.numpy_augment(lambda: self.session.run(data_iterator))\n",
        "\n",
        "    def train(self, train_nimg, report_nimg):\n",
        "        if FLAGS.eval_ckpt:\n",
        "            self.eval_checkpoint(FLAGS.eval_ckpt)\n",
        "            return\n",
        "        batch = FLAGS.batch\n",
        "        train_labeled = self.dataset.train_labeled.repeat().shuffle(FLAGS.shuffle).parse().augment()\n",
        "        train_labeled = train_labeled.batch(batch).prefetch(16).make_one_shot_iterator().get_next()\n",
        "        train_unlabeled = self.dataset.train_unlabeled.repeat().shuffle(FLAGS.shuffle).parse().augment()\n",
        "        train_unlabeled = train_unlabeled.batch(batch).prefetch(16).make_one_shot_iterator().get_next()\n",
        "        scaffold = tf.train.Scaffold(saver=tf.train.Saver(max_to_keep=FLAGS.keep_ckpt, pad_step_number=10))\n",
        "\n",
        "        with tf.Session(config=get_config()) as sess:\n",
        "            self.session = sess\n",
        "            self.cache_eval()\n",
        "\n",
        "        with tf.train.MonitoredTrainingSession(\n",
        "                scaffold=scaffold,\n",
        "                checkpoint_dir=self.checkpoint_dir,\n",
        "                config=get_config(),\n",
        "                save_checkpoint_steps=FLAGS.save_kimg << 10,\n",
        "                save_summaries_steps=report_nimg - batch) as train_session:\n",
        "            self.session = train_session._tf_sess()\n",
        "            gen_labeled = self.gen_labeled_fn(train_labeled)\n",
        "            gen_unlabeled = self.gen_unlabeled_fn(train_unlabeled)\n",
        "            self.tmp.step = self.session.run(self.step)\n",
        "            while self.tmp.step < train_nimg:\n",
        "                loop = trange(self.tmp.step % report_nimg, report_nimg, batch,\n",
        "                              leave=False, unit='img', unit_scale=batch,\n",
        "                              desc='Epoch %d/%d' % (1 + (self.tmp.step // report_nimg), train_nimg // report_nimg))\n",
        "                for _ in loop:\n",
        "                    self.train_step(train_session, gen_labeled, gen_unlabeled)\n",
        "                    while self.tmp.print_queue:\n",
        "                        loop.write(self.tmp.print_queue.pop(0))\n",
        "            while self.tmp.print_queue:\n",
        "                print(self.tmp.print_queue.pop(0))\n",
        "\n",
        "    def eval_checkpoint(self, ckpt=None):\n",
        "        self.eval_mode(ckpt)\n",
        "        self.cache_eval()\n",
        "        raw = self.eval_stats(classify_op=self.ops.classify_raw)\n",
        "        ema = self.eval_stats(classify_op=self.ops.classify_op)\n",
        "        print('%16s %8s %8s %8s' % ('', 'labeled', 'valid', 'test'))\n",
        "        print('%16s %8s %8s %8s' % (('raw',) + tuple('%.2f' % x for x in raw)))\n",
        "        print('%16s %8s %8s %8s' % (('ema',) + tuple('%.2f' % x for x in ema)))\n",
        "\n",
        "    def cache_eval(self):\n",
        "        \"\"\"Cache datasets for computing eval stats.\"\"\"\n",
        "\n",
        "        def collect_samples(dataset, name):\n",
        "            \"\"\"Return numpy arrays of all the samples from a dataset.\"\"\"\n",
        "            pbar = tqdm(desc='Caching %s examples' % name)\n",
        "            it = dataset.batch(1).prefetch(16).make_one_shot_iterator().get_next()\n",
        "            images, labels = [], []\n",
        "            while 1:\n",
        "                try:\n",
        "                    v = self.session.run(it)\n",
        "                except tf.errors.OutOfRangeError:\n",
        "                    break\n",
        "                images.append(v['image'])\n",
        "                labels.append(v['label'])\n",
        "                pbar.update()\n",
        "\n",
        "            images = np.concatenate(images, axis=0)\n",
        "            labels = np.concatenate(labels, axis=0)\n",
        "            pbar.close()\n",
        "            return images, labels\n",
        "\n",
        "        if 'test' not in self.tmp.cache:\n",
        "            self.tmp.cache.test = collect_samples(self.dataset.test.parse(), name='test')\n",
        "            self.tmp.cache.valid = collect_samples(self.dataset.valid.parse(), name='valid')\n",
        "            self.tmp.cache.train_labeled = collect_samples(self.dataset.train_labeled.take(10000).parse(),\n",
        "                                                           name='train_labeled')\n",
        "\n",
        "    def eval_stats(self, batch=None, feed_extra=None, classify_op=None, verbose=True):\n",
        "        \"\"\"Evaluate model on train, valid and test.\"\"\"\n",
        "        batch = batch or FLAGS.batch\n",
        "        classify_op = self.ops.classify_op if classify_op is None else classify_op\n",
        "        accuracies = []\n",
        "        for subset in ('train_labeled', 'valid', 'test'):\n",
        "            images, labels = self.tmp.cache[subset]\n",
        "            predicted = []\n",
        "\n",
        "            for x in range(0, images.shape[0], batch):\n",
        "                p = self.session.run(\n",
        "                    classify_op,\n",
        "                    feed_dict={\n",
        "                        self.ops.x: images[x:x + batch],\n",
        "                        **(feed_extra or {})\n",
        "                    })\n",
        "                predicted.append(p)\n",
        "            predicted = np.concatenate(predicted, axis=0)\n",
        "            accuracies.append((predicted.argmax(1) == labels).mean() * 100)\n",
        "        if verbose:\n",
        "            self.train_print('kimg %-5d  accuracy train/valid/test  %.2f  %.2f  %.2f' %\n",
        "                             tuple([self.tmp.step >> 10] + accuracies))\n",
        "        return np.array(accuracies, 'f')\n",
        "\n",
        "    def add_summaries(self, feed_extra=None, **kwargs):\n",
        "        del kwargs\n",
        "\n",
        "        def gen_stats(classify_op=None, verbose=True):\n",
        "            return self.eval_stats(feed_extra=feed_extra, classify_op=classify_op, verbose=verbose)\n",
        "\n",
        "        accuracies = tf.py_func(functools.partial(gen_stats), [], tf.float32)\n",
        "        tf.summary.scalar('accuracy/train_labeled', accuracies[0])\n",
        "        tf.summary.scalar('accuracy/valid', accuracies[1])\n",
        "        tf.summary.scalar('accuracy', accuracies[2])\n",
        "        if 'classify_raw' in self.ops:\n",
        "            accuracies = tf.py_func(functools.partial(gen_stats,\n",
        "                                                      classify_op=self.ops.classify_raw,\n",
        "                                                      verbose=False), [], tf.float32)\n",
        "            tf.summary.scalar('accuracy/raw/train_labeled', accuracies[0])\n",
        "            tf.summary.scalar('accuracy/raw/valid', accuracies[1])\n",
        "            tf.summary.scalar('accuracy/raw', accuracies[2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4tEX7wZgQmm"
      },
      "source": [
        "# fully_supervised\\lib\\train.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import tensorflow as tf\n",
        "from absl import flags\n",
        "from tqdm import trange\n",
        "\n",
        "# from libml import utils\n",
        "# from libml.train import ClassifySemi\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "class ClassifyFullySupervised(ClassifySemi):\n",
        "    \"\"\"Fully supervised classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def train_step(self, train_session, gen_labeled):\n",
        "        x = gen_labeled()\n",
        "        self.tmp.step = train_session.run([self.ops.train_op, self.ops.update_step],\n",
        "                                          feed_dict={self.ops.xt: x['image'],\n",
        "                                                     self.ops.label: x['label']})[1]\n",
        "\n",
        "    def train(self, train_nimg, report_nimg):\n",
        "        if FLAGS.eval_ckpt:\n",
        "            self.eval_checkpoint(FLAGS.eval_ckpt)\n",
        "            return\n",
        "        batch = FLAGS.batch\n",
        "        train_labeled = self.dataset.train_labeled.repeat().shuffle(FLAGS.shuffle).parse().augment()\n",
        "        train_labeled = train_labeled.batch(batch).prefetch(16).make_one_shot_iterator().get_next()\n",
        "        scaffold = tf.train.Scaffold(saver=tf.train.Saver(max_to_keep=FLAGS.keep_ckpt, pad_step_number=10))\n",
        "\n",
        "        with tf.Session(config=get_config()) as sess:\n",
        "            self.session = sess\n",
        "            self.cache_eval()\n",
        "\n",
        "        with tf.train.MonitoredTrainingSession(\n",
        "                scaffold=scaffold,\n",
        "                checkpoint_dir=self.checkpoint_dir,\n",
        "                config=get_config(),\n",
        "                save_checkpoint_steps=FLAGS.save_kimg << 10,\n",
        "                save_summaries_steps=report_nimg - batch) as train_session:\n",
        "            self.session = train_session._tf_sess()\n",
        "            gen_labeled = self.gen_labeled_fn(train_labeled)\n",
        "            self.tmp.step = self.session.run(self.step)\n",
        "            while self.tmp.step < train_nimg:\n",
        "                loop = trange(self.tmp.step % report_nimg, report_nimg, batch,\n",
        "                              leave=False, unit='img', unit_scale=batch,\n",
        "                              desc='Epoch %d/%d' % (1 + (self.tmp.step // report_nimg), train_nimg // report_nimg))\n",
        "                for _ in loop:\n",
        "                    self.train_step(train_session, gen_labeled)\n",
        "                    while self.tmp.print_queue:\n",
        "                        loop.write(self.tmp.print_queue.pop(0))\n",
        "            while self.tmp.print_queue:\n",
        "                print(self.tmp.print_queue.pop(0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqPLWKejf82l"
      },
      "source": [
        "# cta\\lib\\train.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import numpy as np\n",
        "from absl import flags\n",
        "\n",
        "# from fully_supervised.lib.train import ClassifyFullySupervised\n",
        "# from libml import data\n",
        "# from libml.augment import AugmentPoolCTA\n",
        "# from libml.ctaugment import CTAugment\n",
        "# from libml.train import ClassifySemi\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "for name in list(flags.FLAGS):\n",
        "    if name == 'adepth' or name == 'adecay' or name == 'ath':\n",
        "      delattr(flags.FLAGS,name) \n",
        "\n",
        "\n",
        "flags.DEFINE_integer('adepth', 2, 'Augmentation depth.')\n",
        "flags.DEFINE_float('adecay', 0.99, 'Augmentation decay.')\n",
        "flags.DEFINE_float('ath', 0.80, 'Augmentation threshold.')\n",
        "\n",
        "\n",
        "class CTAClassifySemi(ClassifySemi):\n",
        "    \"\"\"Semi-supervised classification.\"\"\"\n",
        "    AUGMENTER_CLASS = CTAugment\n",
        "    AUGMENT_POOL_CLASS = AugmentPoolCTA\n",
        "\n",
        "    @classmethod\n",
        "    def cta_name(cls):\n",
        "        return '%s_depth%d_th%.2f_decay%.3f' % (cls.AUGMENTER_CLASS.__name__,\n",
        "                                                FLAGS.adepth, FLAGS.ath, FLAGS.adecay)\n",
        "\n",
        "    def __init__(self, train_dir: str, dataset: DataSets, nclass: int, **kwargs):\n",
        "        ClassifySemi.__init__(self, train_dir, dataset, nclass, **kwargs)\n",
        "        self.augmenter = self.AUGMENTER_CLASS(FLAGS.adepth, FLAGS.ath, FLAGS.adecay)\n",
        "\n",
        "    def gen_labeled_fn(self, data_iterator):\n",
        "        def wrap():\n",
        "            batch = self.session.run(data_iterator)\n",
        "            batch['cta'] = self.augmenter\n",
        "            batch['probe'] = True\n",
        "            return batch\n",
        "\n",
        "        return self.AUGMENT_POOL_CLASS(wrap)\n",
        "\n",
        "    def gen_unlabeled_fn(self, data_iterator):\n",
        "        def wrap():\n",
        "            batch = self.session.run(data_iterator)\n",
        "            batch['cta'] = self.augmenter\n",
        "            batch['probe'] = False\n",
        "            return batch\n",
        "\n",
        "        return self.AUGMENT_POOL_CLASS(wrap)\n",
        "\n",
        "    def train_step(self, train_session, gen_labeled, gen_unlabeled):\n",
        "        x, y = gen_labeled(), gen_unlabeled()\n",
        "        v = train_session.run([self.ops.classify_op, self.ops.train_op, self.ops.update_step],\n",
        "                              feed_dict={self.ops.y: y['image'],\n",
        "                                         self.ops.x: x['probe'],\n",
        "                                         self.ops.xt: x['image'],\n",
        "                                         self.ops.label: x['label']})\n",
        "        self.tmp.step = v[-1]\n",
        "        lx = v[0]\n",
        "        for p in range(lx.shape[0]):\n",
        "            error = lx[p]\n",
        "            error[x['label'][p]] -= 1\n",
        "            error = np.abs(error).sum()\n",
        "            self.augmenter.update_rates(x['policy'][p], 1 - 0.5 * error)\n",
        "\n",
        "    def eval_stats(self, batch=None, feed_extra=None, classify_op=None, verbose=True):\n",
        "        \"\"\"Evaluate model on train, valid and test.\"\"\"\n",
        "        batch = batch or FLAGS.batch\n",
        "        classify_op = self.ops.classify_op if classify_op is None else classify_op\n",
        "        accuracies = []\n",
        "        for subset in ('train_labeled', 'valid', 'test'):\n",
        "            images, labels = self.tmp.cache[subset]\n",
        "            predicted = []\n",
        "\n",
        "            for x in range(0, images.shape[0], batch):\n",
        "                p = self.session.run(\n",
        "                    classify_op,\n",
        "                    feed_dict={\n",
        "                        self.ops.x: images[x:x + batch],\n",
        "                        **(feed_extra or {})\n",
        "                    })\n",
        "                predicted.append(p)\n",
        "            predicted = np.concatenate(predicted, axis=0)\n",
        "            accuracies.append((predicted.argmax(1) == labels).mean() * 100)\n",
        "        if verbose:\n",
        "            self.train_print('kimg %-5d  accuracy train/valid/test  %.2f  %.2f  %.2f' %\n",
        "                             tuple([self.tmp.step >> 10] + accuracies))\n",
        "        self.train_print(self.augmenter.stats())\n",
        "        return np.array(accuracies, 'f')\n",
        "\n",
        "\n",
        "class CTAClassifyFullySupervised(ClassifyFullySupervised, CTAClassifySemi):\n",
        "    \"\"\"Fully-supervised classification.\"\"\"\n",
        "\n",
        "    def train_step(self, train_session, gen_labeled):\n",
        "        x = gen_labeled()\n",
        "        v = train_session.run([self.ops.classify_op, self.ops.train_op, self.ops.update_step],\n",
        "                              feed_dict={self.ops.x: x['probe'],\n",
        "                                         self.ops.xt: x['image'],\n",
        "                                         self.ops.label: x['label']})\n",
        "        self.tmp.step = v[-1]\n",
        "        lx = v[0]\n",
        "        for p in range(lx.shape[0]):\n",
        "            error = lx[p]\n",
        "            error[x['label'][p]] -= 1\n",
        "            error = np.abs(error).sum()\n",
        "            self.augmenter.update_rates(x['policy'][p], 1 - 0.5 * error)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LBqqLr9sz_7"
      },
      "source": [
        "# libml\\layers.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Custom neural network layers and primitives.\n",
        "\"\"\"\n",
        "import numbers\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# from libml.data import DataSets\n",
        "\n",
        "\n",
        "def smart_shape(x):\n",
        "    s, t = x.shape, tf.shape(x)\n",
        "    return [t[i] if s[i].value is None else s[i] for i in range(len(s))]\n",
        "\n",
        "\n",
        "def entropy_from_logits(logits):\n",
        "    \"\"\"Computes entropy from classifier logits.\n",
        "\n",
        "    Args:\n",
        "        logits: a tensor of shape (batch_size, class_count) representing the\n",
        "        logits of a classifier.\n",
        "\n",
        "    Returns:\n",
        "        A tensor of shape (batch_size,) of floats giving the entropies\n",
        "        batchwise.\n",
        "    \"\"\"\n",
        "    distribution = tf.contrib.distributions.Categorical(logits=logits)\n",
        "    return distribution.entropy()\n",
        "\n",
        "\n",
        "def entropy_penalty(logits, entropy_penalty_multiplier, mask):\n",
        "    \"\"\"Computes an entropy penalty using the classifier logits.\n",
        "\n",
        "    Args:\n",
        "        logits: a tensor of shape (batch_size, class_count) representing the\n",
        "            logits of a classifier.\n",
        "        entropy_penalty_multiplier: A float by which the entropy is multiplied.\n",
        "        mask: A tensor that optionally masks out some of the costs.\n",
        "\n",
        "    Returns:\n",
        "        The mean entropy penalty\n",
        "    \"\"\"\n",
        "    entropy = entropy_from_logits(logits)\n",
        "    losses = entropy * entropy_penalty_multiplier\n",
        "    losses *= tf.cast(mask, tf.float32)\n",
        "    return tf.reduce_mean(losses)\n",
        "\n",
        "\n",
        "def kl_divergence_from_logits(logits_a, logits_b):\n",
        "    \"\"\"Gets KL divergence from logits parameterizing categorical distributions.\n",
        "\n",
        "    Args:\n",
        "        logits_a: A tensor of logits parameterizing the first distribution.\n",
        "        logits_b: A tensor of logits parameterizing the second distribution.\n",
        "\n",
        "    Returns:\n",
        "        The (batch_size,) shaped tensor of KL divergences.\n",
        "    \"\"\"\n",
        "    distribution1 = tf.contrib.distributions.Categorical(logits=logits_a)\n",
        "    distribution2 = tf.contrib.distributions.Categorical(logits=logits_b)\n",
        "    return tf.contrib.distributions.kl_divergence(distribution1, distribution2)\n",
        "\n",
        "\n",
        "def mse_from_logits(output_logits, target_logits):\n",
        "    \"\"\"Computes MSE between predictions associated with logits.\n",
        "\n",
        "    Args:\n",
        "        output_logits: A tensor of logits from the primary model.\n",
        "        target_logits: A tensor of logits from the secondary model.\n",
        "\n",
        "    Returns:\n",
        "        The mean MSE\n",
        "    \"\"\"\n",
        "    diffs = tf.nn.softmax(output_logits) - tf.nn.softmax(target_logits)\n",
        "    squared_diffs = tf.square(diffs)\n",
        "    return tf.reduce_mean(squared_diffs, -1)\n",
        "\n",
        "\n",
        "def interleave_offsets(batch, nu):\n",
        "    groups = [batch // (nu + 1)] * (nu + 1)\n",
        "    for x in range(batch - sum(groups)):\n",
        "        groups[-x - 1] += 1\n",
        "    offsets = [0]\n",
        "    for g in groups:\n",
        "        offsets.append(offsets[-1] + g)\n",
        "    assert offsets[-1] == batch\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def interleave(xy, batch):\n",
        "    nu = len(xy) - 1\n",
        "    offsets = interleave_offsets(batch, nu)\n",
        "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
        "    for i in range(1, nu + 1):\n",
        "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
        "    return [tf.concat(v, axis=0) for v in xy]\n",
        "\n",
        "\n",
        "def renorm(v):\n",
        "    return v / tf.reduce_sum(v, axis=-1, keepdims=True)\n",
        "\n",
        "\n",
        "def shakeshake(a, b, training):\n",
        "    if not training:\n",
        "        return 0.5 * (a + b)\n",
        "    mu = tf.random_uniform([tf.shape(a)[0]] + [1] * (len(a.shape) - 1), 0, 1)\n",
        "    mixf = a + mu * (b - a)\n",
        "    mixb = a + mu[::1] * (b - a)\n",
        "    return tf.stop_gradient(mixf - mixb) + mixb\n",
        "\n",
        "\n",
        "class PMovingAverage:\n",
        "    def __init__(self, name, nclass, buf_size):\n",
        "        # MEAN aggregation is used by DistributionStrategy to aggregate\n",
        "        # variable updates across shards\n",
        "        self.ma = tf.Variable(tf.ones([buf_size, nclass]) / nclass,\n",
        "                              trainable=False,\n",
        "                              name=name,\n",
        "                              aggregation=tf.VariableAggregation.MEAN)\n",
        "\n",
        "    def __call__(self):\n",
        "        v = tf.reduce_mean(self.ma, axis=0)\n",
        "        return v / tf.reduce_sum(v)\n",
        "\n",
        "    def update(self, entry):\n",
        "        entry = tf.reduce_mean(entry, axis=0)\n",
        "        return tf.assign(self.ma, tf.concat([self.ma[1:], [entry]], axis=0))\n",
        "\n",
        "\n",
        "class PData:\n",
        "    def __init__(self, dataset: DataSets):\n",
        "        self.has_update = False\n",
        "        if dataset.p_unlabeled is not None:\n",
        "            self.p_data = tf.constant(dataset.p_unlabeled, name='p_data')\n",
        "        elif dataset.p_labeled is not None:\n",
        "            self.p_data = tf.constant(dataset.p_labeled, name='p_data')\n",
        "        else:\n",
        "            # MEAN aggregation is used by DistributionStrategy to aggregate\n",
        "            # variable updates across shards\n",
        "            self.p_data = tf.Variable(renorm(tf.ones([dataset.nclass])),\n",
        "                                      trainable=False,\n",
        "                                      name='p_data',\n",
        "                                      aggregation=tf.VariableAggregation.MEAN)\n",
        "            self.has_update = True\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.p_data / tf.reduce_sum(self.p_data)\n",
        "\n",
        "    def update(self, entry, decay=0.999):\n",
        "        entry = tf.reduce_mean(entry, axis=0)\n",
        "        return tf.assign(self.p_data, self.p_data * decay + entry * (1 - decay))\n",
        "\n",
        "\n",
        "class MixMode:\n",
        "    # A class for mixing data for various combination of labeled and unlabeled.\n",
        "    # x = labeled example\n",
        "    # y = unlabeled example\n",
        "    # For example \"xx.yxy\" means: mix x with x, mix y with both x and y.\n",
        "    MODES = 'xx.yy xxy.yxy xx.yxy xx.yx xx. .yy xxy. .yxy .'.split()\n",
        "\n",
        "    def __init__(self, mode):\n",
        "        assert mode in self.MODES\n",
        "        self.mode = mode\n",
        "\n",
        "    @staticmethod\n",
        "    def augment_pair(x0, l0, x1, l1, beta, **kwargs):\n",
        "        del kwargs\n",
        "        if isinstance(beta, numbers.Integral) and beta <= 0:\n",
        "            return x0, l0\n",
        "\n",
        "        def np_beta(s, beta):  # TF implementation seems unreliable for beta below 0.2\n",
        "            return np.random.beta(beta, beta, s).astype('f')\n",
        "\n",
        "        with tf.device('/cpu'):\n",
        "            mix = tf.py_func(np_beta, [tf.shape(x0)[0], beta], tf.float32)\n",
        "            mix = tf.reshape(tf.maximum(mix, 1 - mix), [tf.shape(x0)[0], 1, 1, 1])\n",
        "            index = tf.random_shuffle(tf.range(tf.shape(x0)[0]))\n",
        "        xs = tf.gather(x1, index)\n",
        "        ls = tf.gather(l1, index)\n",
        "        xmix = x0 * mix + xs * (1 - mix)\n",
        "        lmix = l0 * mix[:, :, 0, 0] + ls * (1 - mix[:, :, 0, 0])\n",
        "        return xmix, lmix\n",
        "\n",
        "    @staticmethod\n",
        "    def augment(x, l, beta, **kwargs):\n",
        "        return MixMode.augment_pair(x, l, x, l, beta, **kwargs)\n",
        "\n",
        "    def __call__(self, xl: list, ll: list, betal: list):\n",
        "        assert len(xl) == len(ll) >= 2\n",
        "        assert len(betal) == 2\n",
        "        if self.mode == '.':\n",
        "            return xl, ll\n",
        "        elif self.mode == 'xx.':\n",
        "            mx0, ml0 = self.augment(xl[0], ll[0], betal[0])\n",
        "            return [mx0] + xl[1:], [ml0] + ll[1:]\n",
        "        elif self.mode == '.yy':\n",
        "            mx1, ml1 = self.augment(\n",
        "                tf.concat(xl[1:], 0), tf.concat(ll[1:], 0), betal[1])\n",
        "            return (xl[:1] + tf.split(mx1, len(xl) - 1),\n",
        "                    ll[:1] + tf.split(ml1, len(ll) - 1))\n",
        "        elif self.mode == 'xx.yy':\n",
        "            mx0, ml0 = self.augment(xl[0], ll[0], betal[0])\n",
        "            mx1, ml1 = self.augment(\n",
        "                tf.concat(xl[1:], 0), tf.concat(ll[1:], 0), betal[1])\n",
        "            return ([mx0] + tf.split(mx1, len(xl) - 1),\n",
        "                    [ml0] + tf.split(ml1, len(ll) - 1))\n",
        "        elif self.mode == 'xxy.':\n",
        "            mx, ml = self.augment(\n",
        "                tf.concat(xl, 0), tf.concat(ll, 0),\n",
        "                sum(betal) / len(betal))\n",
        "            return (tf.split(mx, len(xl))[:1] + xl[1:],\n",
        "                    tf.split(ml, len(ll))[:1] + ll[1:])\n",
        "        elif self.mode == '.yxy':\n",
        "            mx, ml = self.augment(\n",
        "                tf.concat(xl, 0), tf.concat(ll, 0),\n",
        "                sum(betal) / len(betal))\n",
        "            return (xl[:1] + tf.split(mx, len(xl))[1:],\n",
        "                    ll[:1] + tf.split(ml, len(ll))[1:])\n",
        "        elif self.mode == 'xxy.yxy':\n",
        "            mx, ml = self.augment(\n",
        "                tf.concat(xl, 0), tf.concat(ll, 0),\n",
        "                sum(betal) / len(betal))\n",
        "            return tf.split(mx, len(xl)), tf.split(ml, len(ll))\n",
        "        elif self.mode == 'xx.yxy':\n",
        "            mx0, ml0 = self.augment(xl[0], ll[0], betal[0])\n",
        "            mx1, ml1 = self.augment(tf.concat(xl, 0), tf.concat(ll, 0), betal[1])\n",
        "            mx1, ml1 = [tf.split(m, len(xl))[1:] for m in (mx1, ml1)]\n",
        "            return [mx0] + mx1, [ml0] + ml1\n",
        "        elif self.mode == 'xx.yx':\n",
        "            mx0, ml0 = self.augment(xl[0], ll[0], betal[0])\n",
        "            mx1, ml1 = zip(*[\n",
        "                self.augment_pair(xl[i], ll[i], xl[0], ll[0], betal[1])\n",
        "                for i in range(1, len(xl))\n",
        "            ])\n",
        "            return [mx0] + list(mx1), [ml0] + list(ml1)\n",
        "        raise NotImplementedError(self.mode)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RcWITDmtFWL"
      },
      "source": [
        "# libml\\models.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Classifier architectures.\"\"\"\n",
        "import functools\n",
        "import itertools\n",
        "\n",
        "import tensorflow as tf\n",
        "from absl import flags\n",
        "\n",
        "# from libml import layers\n",
        "# from libml.train import ClassifySemi\n",
        "# from libml.utils import EasyDict\n",
        "\n",
        "\n",
        "class CNN13(ClassifySemi):\n",
        "    \"\"\"Simplified reproduction of the Mean Teacher paper network. filters=128 in original implementation.\n",
        "    Removed dropout, Gaussians, forked dense layers, basically all non-standard things.\"\"\"\n",
        "\n",
        "    def classifier(self, x, scales, filters, training, getter=None, **kwargs):\n",
        "        del kwargs\n",
        "        assert scales == 3  # Only specified for 32x32 inputs.\n",
        "        conv_args = dict(kernel_size=3, activation=tf.nn.leaky_relu, padding='same')\n",
        "        bn_args = dict(training=training, momentum=0.999)\n",
        "\n",
        "        with tf.variable_scope('classify', reuse=tf.AUTO_REUSE, custom_getter=getter):\n",
        "            y = tf.layers.conv2d((x - self.dataset.mean) / self.dataset.std, filters, **conv_args)\n",
        "            y = tf.layers.batch_normalization(y, **bn_args)\n",
        "            y = tf.layers.conv2d(y, filters, **conv_args)\n",
        "            y = tf.layers.batch_normalization(y, **bn_args)\n",
        "            y = tf.layers.conv2d(y, filters, **conv_args)\n",
        "            y = tf.layers.batch_normalization(y, **bn_args)\n",
        "            y = tf.layers.max_pooling2d(y, 2, 2)\n",
        "            y = tf.layers.conv2d(y, 2 * filters, **conv_args)\n",
        "            y = tf.layers.batch_normalization(y, **bn_args)\n",
        "            y = tf.layers.conv2d(y, 2 * filters, **conv_args)\n",
        "            y = tf.layers.batch_normalization(y, **bn_args)\n",
        "            y = tf.layers.conv2d(y, 2 * filters, **conv_args)\n",
        "            y = tf.layers.batch_normalization(y, **bn_args)\n",
        "            y = tf.layers.max_pooling2d(y, 2, 2)\n",
        "            y = tf.layers.conv2d(y, 4 * filters, kernel_size=3, activation=tf.nn.leaky_relu, padding='valid')\n",
        "            y = tf.layers.batch_normalization(y, **bn_args)\n",
        "            y = tf.layers.conv2d(y, 2 * filters, kernel_size=1, activation=tf.nn.leaky_relu, padding='same')\n",
        "            y = tf.layers.batch_normalization(y, **bn_args)\n",
        "            y = tf.layers.conv2d(y, 1 * filters, kernel_size=1, activation=tf.nn.leaky_relu, padding='same')\n",
        "            y = tf.layers.batch_normalization(y, **bn_args)\n",
        "            y = tf.reduce_mean(y, [1, 2])  # (b, 6, 6, 128) -> (b, 128)\n",
        "            logits = tf.layers.dense(y, self.nclass)\n",
        "        return EasyDict(logits=logits, embeds=y)\n",
        "\n",
        "\n",
        "class ResNet(ClassifySemi):\n",
        "    def classifier(self, x, scales, filters, repeat, training, getter=None, dropout=0, **kwargs):\n",
        "        del kwargs\n",
        "        leaky_relu = functools.partial(tf.nn.leaky_relu, alpha=0.1)\n",
        "        bn_args = dict(training=training, momentum=0.999)\n",
        "\n",
        "        def conv_args(k, f):\n",
        "            return dict(padding='same',\n",
        "                        kernel_initializer=tf.random_normal_initializer(stddev=tf.rsqrt(0.5 * k * k * f)))\n",
        "\n",
        "        def residual(x0, filters, stride=1, activate_before_residual=False):\n",
        "            x = leaky_relu(tf.layers.batch_normalization(x0, **bn_args))\n",
        "            if activate_before_residual:\n",
        "                x0 = x\n",
        "\n",
        "            x = tf.layers.conv2d(x, filters, 3, strides=stride, **conv_args(3, filters))\n",
        "            x = leaky_relu(tf.layers.batch_normalization(x, **bn_args))\n",
        "            x = tf.layers.conv2d(x, filters, 3, **conv_args(3, filters))\n",
        "\n",
        "            if x0.get_shape()[3] != filters:\n",
        "                x0 = tf.layers.conv2d(x0, filters, 1, strides=stride, **conv_args(1, filters))\n",
        "\n",
        "            return x0 + x\n",
        "\n",
        "        with tf.variable_scope('classify', reuse=tf.AUTO_REUSE, custom_getter=getter):\n",
        "            y = tf.layers.conv2d((x - self.dataset.mean) / self.dataset.std, 16, 3, **conv_args(3, 16))\n",
        "            for scale in range(scales):\n",
        "                y = residual(y, filters << scale, stride=2 if scale else 1, activate_before_residual=scale == 0)\n",
        "                for i in range(repeat - 1):\n",
        "                    y = residual(y, filters << scale)\n",
        "\n",
        "            y = leaky_relu(tf.layers.batch_normalization(y, **bn_args))\n",
        "            y = embeds = tf.reduce_mean(y, [1, 2])\n",
        "            if dropout and training:\n",
        "                y = tf.nn.dropout(y, 1 - dropout)\n",
        "            logits = tf.layers.dense(y, self.nclass, kernel_initializer=tf.glorot_normal_initializer())\n",
        "        return EasyDict(logits=logits, embeds=embeds)\n",
        "\n",
        "\n",
        "class ShakeNet(ClassifySemi):\n",
        "    def classifier(self, x, scales, filters, repeat, training, getter=None, dropout=0, **kwargs):\n",
        "        del kwargs\n",
        "        bn_args = dict(training=training, momentum=0.999)\n",
        "\n",
        "        def conv_args(k, f):\n",
        "            return dict(padding='same', use_bias=False,\n",
        "                        kernel_initializer=tf.random_normal_initializer(stddev=tf.rsqrt(0.5 * k * k * f)))\n",
        "\n",
        "        def residual(x0, filters, stride=1):\n",
        "            def branch():\n",
        "                x = tf.nn.relu(x0)\n",
        "                x = tf.layers.conv2d(x, filters, 3, strides=stride, **conv_args(3, filters))\n",
        "                x = tf.nn.relu(tf.layers.batch_normalization(x, **bn_args))\n",
        "                x = tf.layers.conv2d(x, filters, 3, **conv_args(3, filters))\n",
        "                x = tf.layers.batch_normalization(x, **bn_args)\n",
        "                return x\n",
        "\n",
        "            x = layers.shakeshake(branch(), branch(), training)\n",
        "\n",
        "            if stride == 2:\n",
        "                x1 = tf.layers.conv2d(tf.nn.relu(x0[:, ::2, ::2]), filters >> 1, 1, **conv_args(1, filters >> 1))\n",
        "                x2 = tf.layers.conv2d(tf.nn.relu(x0[:, 1::2, 1::2]), filters >> 1, 1, **conv_args(1, filters >> 1))\n",
        "                x0 = tf.concat([x1, x2], axis=3)\n",
        "                x0 = tf.layers.batch_normalization(x0, **bn_args)\n",
        "            elif x0.get_shape()[3] != filters:\n",
        "                x0 = tf.layers.conv2d(x0, filters, 1, **conv_args(1, filters))\n",
        "                x0 = tf.layers.batch_normalization(x0, **bn_args)\n",
        "\n",
        "            return x0 + x\n",
        "\n",
        "        with tf.variable_scope('classify', reuse=tf.AUTO_REUSE, custom_getter=getter):\n",
        "            y = tf.layers.conv2d((x - self.dataset.mean) / self.dataset.std, 16, 3, **conv_args(3, 16))\n",
        "            for scale, i in itertools.product(range(scales), range(repeat)):\n",
        "                with tf.variable_scope('layer%d.%d' % (scale + 1, i)):\n",
        "                    if i == 0:\n",
        "                        y = residual(y, filters << scale, stride=2 if scale else 1)\n",
        "                    else:\n",
        "                        y = residual(y, filters << scale)\n",
        "\n",
        "            y = embeds = tf.reduce_mean(y, [1, 2])\n",
        "            if dropout and training:\n",
        "                y = tf.nn.dropout(y, 1 - dropout)\n",
        "            logits = tf.layers.dense(y, self.nclass, kernel_initializer=tf.glorot_normal_initializer())\n",
        "        return EasyDict(logits=logits, embeds=embeds)\n",
        "\n",
        "\n",
        "class MultiModel(CNN13, ResNet, ShakeNet):\n",
        "    MODELS = ('cnn13', 'resnet', 'shake')\n",
        "    MODEL_CNN13, MODEL_RESNET, MODEL_SHAKE = MODELS\n",
        "\n",
        "    def augment(self, x, l, smoothing, **kwargs):\n",
        "        del kwargs\n",
        "        return x, l - smoothing * (l - 1. / self.nclass)\n",
        "\n",
        "    def classifier(self, x, arch, **kwargs):\n",
        "        if arch == self.MODEL_CNN13:\n",
        "            return CNN13.classifier(self, x, **kwargs)\n",
        "        elif arch == self.MODEL_RESNET:\n",
        "            return ResNet.classifier(self, x, **kwargs)\n",
        "        elif arch == self.MODEL_SHAKE:\n",
        "            return ShakeNet.classifier(self, x, **kwargs)\n",
        "        raise ValueError('Model %s does not exists, available ones are %s' % (arch, self.MODELS))\n",
        "\n",
        "\n",
        "for name in list(flags.FLAGS):\n",
        "    if name == 'arch':\n",
        "      delattr(flags.FLAGS,name) \n",
        "\n",
        "flags.DEFINE_enum('arch', MultiModel.MODEL_RESNET, MultiModel.MODELS, 'Architecture.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRCSQeGQ47_C"
      },
      "source": [
        "# cta/cta_remixmatch.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import os\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "from cta.lib.train import CTAClassifySemi\n",
        "from libml import utils, data\n",
        "from remixmatch_no_cta import ReMixMatch\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "class CTAReMixMatch(ReMixMatch, CTAClassifySemi):\n",
        "    pass\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "    utils.setup_main()\n",
        "    del argv  # Unused.\n",
        "    dataset = data.MANY_DATASETS()[FLAGS.dataset]()\n",
        "    log_width = utils.ilog2(dataset.width)\n",
        "    model = CTAReMixMatch(\n",
        "        os.path.join(FLAGS.train_dir, dataset.name, CTAReMixMatch.cta_name()),\n",
        "        dataset,\n",
        "        lr=FLAGS.lr,\n",
        "        wd=FLAGS.wd,\n",
        "        arch=FLAGS.arch,\n",
        "        batch=FLAGS.batch,\n",
        "        nclass=dataset.nclass,\n",
        "\n",
        "        K=FLAGS.K,\n",
        "        beta=FLAGS.beta,\n",
        "        w_kl=FLAGS.w_kl,\n",
        "        w_match=FLAGS.w_match,\n",
        "        w_rot=FLAGS.w_rot,\n",
        "        redux=FLAGS.redux,\n",
        "        use_dm=FLAGS.use_dm,\n",
        "        use_xe=FLAGS.use_xe,\n",
        "        warmup_kimg=FLAGS.warmup_kimg,\n",
        "\n",
        "        scales=FLAGS.scales or (log_width - 2),\n",
        "        filters=FLAGS.filters,\n",
        "        repeat=FLAGS.repeat)\n",
        "    model.train(FLAGS.train_kimg << 10, FLAGS.report_kimg << 10)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    utils.setup_tf()\n",
        "    flags.DEFINE_float('wd', 0.02, 'Weight decay.')\n",
        "    flags.DEFINE_float('beta', 0.75, 'Mixup beta distribution.')\n",
        "    flags.DEFINE_float('w_kl', 0.5, 'Weight for KL loss.')\n",
        "    flags.DEFINE_float('w_match', 1.5, 'Weight for distribution matching loss.')\n",
        "    flags.DEFINE_float('w_rot', 0.5, 'Weight for rotation loss.')\n",
        "    flags.DEFINE_integer('scales', 0, 'Number of 2x2 downscalings in the classifier.')\n",
        "    flags.DEFINE_integer('filters', 32, 'Filter size of convolutions.')\n",
        "    flags.DEFINE_integer('repeat', 4, 'Number of residual layers per stage.')\n",
        "    flags.DEFINE_integer('warmup_kimg', 1024, 'Unannealing duration for SSL loss.')\n",
        "    flags.DEFINE_enum('redux', '1st', 'swap mean 1st'.split(), 'Logit selection.')\n",
        "    flags.DEFINE_bool('use_dm', True, 'Whether to use distribution matching.')\n",
        "    flags.DEFINE_bool('use_xe', True, 'Whether to use cross-entropy or Brier.')\n",
        "    FLAGS.set_default('augment', 'd.d.d')\n",
        "    FLAGS.set_default('dataset', 'cifar10.3@250-5000')\n",
        "    FLAGS.set_default('batch', 64)\n",
        "    FLAGS.set_default('lr', 0.002)\n",
        "    FLAGS.set_default('train_kimg', 1 << 16)\n",
        "    app.run(main)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqm2MpL_V5U5"
      },
      "source": [
        "# fixmatch.py\n",
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "import functools\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from tqdm import trange\n",
        "\n",
        "# from cta.cta_remixmatch import CTAReMixMatch\n",
        "# from libml import data, utils, augment, ctaugment\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "class AugmentPoolCTACutOut(augment.AugmentPoolCTA):\n",
        "    @staticmethod\n",
        "    def numpy_apply_policies(arglist):\n",
        "        x, cta, probe = arglist\n",
        "        if x.ndim == 3:\n",
        "            assert probe\n",
        "            policy = policy(probe=True)\n",
        "            return dict(policy=policy,\n",
        "                        probe=ctaugment.apply(x, policy),\n",
        "                        image=x)\n",
        "        assert not probe\n",
        "        cutout_policy = lambda: cta.policy(probe=False) + [ctaugment.OP('cutout', (1,))]\n",
        "        return dict(image=np.stack([x[0]] + [ctaugment.apply(y, cutout_policy()) for y in x[1:]]).astype('f'))\n",
        "\n",
        "\n",
        "class FixMatch(CTAReMixMatch):\n",
        "    if FLAGS.augment_improve:\n",
        "      AUGMENT_POOL_CLASS = AugmentPoolCTACutOut\n",
        "    else:\n",
        "      AUGMENT_POOL_CLASS = None     # not to use augmentation improvement\n",
        "    \n",
        "    def train(self, train_nimg, report_nimg):\n",
        "        if FLAGS.eval_ckpt:\n",
        "            self.eval_checkpoint(FLAGS.eval_ckpt)\n",
        "            return\n",
        "        batch = FLAGS.batch\n",
        "        train_labeled = self.dataset.train_labeled.repeat().shuffle(FLAGS.shuffle).parse().augment()\n",
        "        train_labeled = train_labeled.batch(batch).prefetch(16).make_one_shot_iterator().get_next()\n",
        "        train_unlabeled = self.dataset.train_unlabeled.repeat().shuffle(FLAGS.shuffle).parse().augment()\n",
        "        train_unlabeled = train_unlabeled.batch(batch * self.params['uratio']).prefetch(16)\n",
        "        train_unlabeled = train_unlabeled.make_one_shot_iterator().get_next()\n",
        "        scaffold = tf.train.Scaffold(saver=tf.train.Saver(max_to_keep=FLAGS.keep_ckpt,\n",
        "                                                          pad_step_number=10))\n",
        "\n",
        "        with tf.Session(config=get_config()) as sess:\n",
        "            self.session = sess\n",
        "            self.cache_eval()\n",
        "\n",
        "        with tf.train.MonitoredTrainingSession(\n",
        "                scaffold=scaffold,\n",
        "                checkpoint_dir=self.checkpoint_dir,\n",
        "                config=get_config(),\n",
        "                save_checkpoint_steps=FLAGS.save_kimg << 10,\n",
        "                save_summaries_steps=report_nimg - batch) as train_session:\n",
        "            self.session = train_session._tf_sess()\n",
        "            gen_labeled = self.gen_labeled_fn(train_labeled)\n",
        "            gen_unlabeled = self.gen_unlabeled_fn(train_unlabeled)\n",
        "            self.tmp.step = self.session.run(self.step)\n",
        "            while self.tmp.step < train_nimg:\n",
        "                loop = trange(self.tmp.step % report_nimg, report_nimg, batch,\n",
        "                              leave=False, unit='img', unit_scale=batch,\n",
        "                              desc='Epoch %d/%d' % (1 + (self.tmp.step // report_nimg), train_nimg // report_nimg))\n",
        "                for _ in loop:\n",
        "                    self.train_step(train_session, gen_labeled, gen_unlabeled)\n",
        "                    while self.tmp.print_queue:\n",
        "                        loop.write(self.tmp.print_queue.pop(0))\n",
        "            while self.tmp.print_queue:\n",
        "                print(self.tmp.print_queue.pop(0))\n",
        "\n",
        "    def model(self, batch, lr, wd, wu, confidence, uratio, ema=0.999, **kwargs):\n",
        "        hwc = [self.dataset.height, self.dataset.width, self.dataset.colors]\n",
        "        xt_in = tf.placeholder(tf.float32, [batch] + hwc, 'xt')  # Training labeled\n",
        "        x_in = tf.placeholder(tf.float32, [None] + hwc, 'x')  # Eval images\n",
        "        y_in = tf.placeholder(tf.float32, [batch * uratio, 2] + hwc, 'y')  # Training unlabeled (weak, strong)\n",
        "        l_in = tf.placeholder(tf.int32, [batch], 'labels')  # Labels\n",
        "\n",
        "        lrate = tf.clip_by_value(tf.to_float(self.step) / (FLAGS.train_kimg << 10), 0, 1)\n",
        "        lr *= tf.cos(lrate * (7 * np.pi) / (2 * 8))\n",
        "        tf.summary.scalar('monitors/lr', lr)\n",
        "\n",
        "        # Compute logits for xt_in and y_in\n",
        "        classifier = lambda x, **kw: self.classifier(x, **kw, **kwargs).logits\n",
        "        skip_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        x = interleave(tf.concat([xt_in, y_in[:, 0], y_in[:, 1]], 0), 2 * uratio + 1)\n",
        "        logits = para_cat(lambda x: classifier(x, training=True), x)\n",
        "        logits = de_interleave(logits, 2 * uratio+1)\n",
        "        post_ops = [v for v in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if v not in skip_ops]\n",
        "        logits_x = logits[:batch]\n",
        "        logits_weak, logits_strong = tf.split(logits[batch:], 2)\n",
        "        del logits, skip_ops\n",
        "\n",
        "        # Labeled cross-entropy\n",
        "        loss_xe = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=l_in, logits=logits_x)\n",
        "        loss_xe = tf.reduce_mean(loss_xe)\n",
        "        tf.summary.scalar('losses/xe', loss_xe)\n",
        "\n",
        "        # Pseudo-label cross entropy for unlabeled data\n",
        "        pseudo_labels = tf.stop_gradient(tf.nn.softmax(logits_weak))\n",
        "        loss_xeu = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(pseudo_labels, axis=1),\n",
        "                                                                  logits=logits_strong)\n",
        "        pseudo_mask = tf.to_float(tf.reduce_max(pseudo_labels, axis=1) >= confidence)\n",
        "        tf.summary.scalar('monitors/mask', tf.reduce_mean(pseudo_mask))\n",
        "        loss_xeu = tf.reduce_mean(loss_xeu * pseudo_mask)\n",
        "        tf.summary.scalar('losses/xeu', loss_xeu)\n",
        "\n",
        "        # L2 regularization\n",
        "        loss_wd = sum(tf.nn.l2_loss(v) for v in model_vars('classify') if 'kernel' in v.name)\n",
        "        tf.summary.scalar('losses/wd', loss_wd)\n",
        "\n",
        "        ema = tf.train.ExponentialMovingAverage(decay=ema)\n",
        "        ema_op = ema.apply(model_vars())\n",
        "        ema_getter = functools.partial(getter_ema, ema)\n",
        "        post_ops.append(ema_op)\n",
        "\n",
        "        train_op = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True).minimize(\n",
        "            loss_xe + wu * loss_xeu + wd * loss_wd, colocate_gradients_with_ops=True)\n",
        "        with tf.control_dependencies([train_op]):\n",
        "            train_op = tf.group(*post_ops)\n",
        "\n",
        "        return EasyDict(\n",
        "            xt=xt_in, x=x_in, y=y_in, label=l_in, train_op=train_op,\n",
        "            classify_raw=tf.nn.softmax(classifier(x_in, training=False)),  # No EMA, for debugging.\n",
        "            classify_op=tf.nn.softmax(classifier(x_in, getter=ema_getter, training=False)))\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "    setup_main()\n",
        "    del argv  # Unused.\n",
        "    dataset = data.PAIR_DATASETS()[FLAGS.dataset]()\n",
        "    log_width = ilog2(dataset.width)\n",
        "    model = FixMatch(\n",
        "        os.path.join(FLAGS.train_dir, dataset.name, FixMatch.cta_name()),\n",
        "        dataset,\n",
        "        lr=FLAGS.lr,\n",
        "        wd=FLAGS.wd,\n",
        "        arch=FLAGS.arch,\n",
        "        batch=FLAGS.batch,\n",
        "        nclass=dataset.nclass,\n",
        "        wu=FLAGS.wu,\n",
        "        confidence=FLAGS.confidence,\n",
        "        uratio=FLAGS.uratio,\n",
        "        scales=FLAGS.scales or (log_width - 2),\n",
        "        filters=FLAGS.filters,\n",
        "        repeat=FLAGS.repeat)\n",
        "    model.train(FLAGS.train_kimg << 10, FLAGS.report_kimg << 10)\n",
        "\n",
        "def getFixMatch_model(INIT_LEARNING_RATE = 0.03, batch_size = 256, flag_AugmentImprove = 0):\n",
        "  setup_tf()\n",
        "  flags.DEFINE_bool('augment_improve',bool(flag_AugmentImprove),'A flag indicates if to use augmentation improvement')\n",
        "  flags.DEFINE_float('confidence', 0.95, 'Confidence threshold.')\n",
        "  flags.DEFINE_float('wd', 0.0005, 'Weight decay.')\n",
        "  flags.DEFINE_float('wu', 1, 'Pseudo label loss weight.')\n",
        "  flags.DEFINE_integer('filters', 32, 'Filter size of convolutions.')\n",
        "  flags.DEFINE_integer('repeat', 4, 'Number of residual layers per stage.')\n",
        "  flags.DEFINE_integer('scales', 0, 'Number of 2x2 downscalings in the classifier.')\n",
        "  flags.DEFINE_integer('uratio', 7, 'Unlabeled batch size ratio.')\n",
        "  FLAGS.set_default('augment', 'd.d.d')\n",
        "  FLAGS.set_default('dataset', 'cifar10.3@250-1')\n",
        "  FLAGS.set_default('batch', batch_size)\n",
        "  FLAGS.set_default('lr', INIT_LEARNING_RATE)\n",
        "  FLAGS.set_default('train_kimg', 1 << 16)\n",
        "  \n",
        "  model = FixMatch(\n",
        "        os.path.join(FLAGS.train_dir, dataset.name, FixMatch.cta_name()),\n",
        "        dataset,\n",
        "        lr=FLAGS.lr,\n",
        "        wd=FLAGS.wd,\n",
        "        arch=FLAGS.arch,\n",
        "        batch=FLAGS.batch,\n",
        "        nclass=dataset.nclass,\n",
        "        wu=FLAGS.wu,\n",
        "        confidence=FLAGS.confidence,\n",
        "        uratio=FLAGS.uratio,\n",
        "        scales=FLAGS.scales or (log_width - 2),\n",
        "        filters=FLAGS.filters,\n",
        "        repeat=FLAGS.repeat)\n",
        "    # model.train(FLAGS.train_kimg << 10, FLAGS.report_kimg << 10)\n",
        "\n",
        "    return model\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   setup_tf()\n",
        "#   flags.DEFINE_float('confidence', 0.95, 'Confidence threshold.')\n",
        "#   flags.DEFINE_float('wd', 0.0005, 'Weight decay.')\n",
        "#   flags.DEFINE_float('wu', 1, 'Pseudo label loss weight.')\n",
        "#   flags.DEFINE_integer('filters', 32, 'Filter size of convolutions.')\n",
        "#   flags.DEFINE_integer('repeat', 4, 'Number of residual layers per stage.')\n",
        "#   flags.DEFINE_integer('scales', 0, 'Number of 2x2 downscalings in the classifier.')\n",
        "#   flags.DEFINE_integer('uratio', 7, 'Unlabeled batch size ratio.')\n",
        "#   FLAGS.set_default('augment', 'd.d.d')\n",
        "#   FLAGS.set_default('dataset', 'cifar10.3@250-1')\n",
        "#   FLAGS.set_default('batch', 64)\n",
        "#   FLAGS.set_default('lr', 0.03)\n",
        "#   FLAGS.set_default('train_kimg', 1 << 16)\n",
        "#   app.run(main)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsK0l_Ra1j_g"
      },
      "source": [
        "#Stage 3 - CNN #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh5TM1_a1i1S"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.datasets import cifar100\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "# NUM_CLASSES = 100 # NUM_CLASSES\n",
        "# y_train = to_categorical(y_train)\n",
        "# y_test = to_categorical(y_test)\n",
        "\n",
        "# x_train = x_train.astype('float32')\n",
        "# x_test = x_test.astype('float32')\n",
        "# x_train /= 255.0\n",
        "# x_test /= 255.0\n",
        "\n",
        "def getCNN_model(x_train, INIT_DROPOUT_RATE = 0.5, MOMENTUM_RATE = 0.9, INIT_LEARNING_RATE = 0.01, L2_DECAY_RATE = 0.0005, batch_size = 256):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(ZeroPadding2D(4, input_shape=x_train.shape[1:]))\n",
        "  # Stack 1:\n",
        "  model.add(Conv2D(384, (3, 3), padding='same', kernel_regularizer=l2(0.01)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  model.add(Dropout(INIT_DROPOUT_RATE))\n",
        "  # Stack 2:\n",
        "  model.add(Conv2D(384, (1, 1), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(384, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(640, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(640, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  model.add(Dropout(INIT_DROPOUT_RATE))\n",
        "  # Stack 3:\n",
        "  model.add(Conv2D(640, (3, 3), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(768, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(768, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(768, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  model.add(Dropout(INIT_DROPOUT_RATE))\n",
        "  # Stack 4:\n",
        "  model.add(Conv2D(768, (1, 1), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(896, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(896, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  model.add(Dropout(INIT_DROPOUT_RATE))\n",
        "  # Stack 5:\n",
        "  model.add(Conv2D(896, (3, 3), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(1024, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(1024, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  model.add(Dropout(INIT_DROPOUT_RATE))\n",
        "  # Stack 6:\n",
        "  model.add(Conv2D(1024, (1, 1), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Conv2D(1152, (2, 2), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  model.add(Dropout(INIT_DROPOUT_RATE))\n",
        "  # Stack 7:\n",
        "  model.add(Conv2D(1152, (1, 1), padding='same', kernel_regularizer=l2(L2_DECAY_RATE)))\n",
        "  model.add(Activation('elu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  model.add(Dropout(INIT_DROPOUT_RATE))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(NUM_CLASSES))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # epochs = 100\n",
        "\n",
        "  model.compile(optimizer='adagrad',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data= (x_test, y_test))\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1FXg0kD4aBX"
      },
      "source": [
        "# Stage 4 - Evaluation\n",
        "---\n",
        " Evaluating the Models accuracy using different metrics \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRaQbi814aSo"
      },
      "source": [
        "#Evaluation Protocol - External 10-fold cross validation + internal 3-fold cross validation \n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# The function receives the model as a parameter, \n",
        "# model is one out of 3 possible values: FixMatch, FixMatch+Improve, CNN\n",
        "# function also receives the Training data (images) and the corresponding lables\n",
        "def nestedCrossValidationAlgorithm(X, y, model):\n",
        "    maxF1 = 0\n",
        "    i = 1\n",
        "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "    for train_ix, test_ix in cv_outer.split(X):  # outer loop for K = 10\n",
        "        print('iteration ', i, 'begin')\n",
        "        i += 1\n",
        "        # split data\n",
        "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "\n",
        "        # configure the cross-validation procedure\n",
        "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        space = dict()\n",
        "        space['mu'] = [4, 5, 6, 7 ,8]\n",
        "        space['lr']= [0.2, 0.002, 0.02, 0.03, 0.3, 0.003,  0.04, 0.4, 0.004] \n",
        "        space['batch_size'] = [64, 128, 256]\n",
        "\n",
        "        # define search\n",
        "        search = RandomizedSearchCV(model, space, n_iter = 50, scoring='f1_macro', cv=cv_inner, refit=True)\n",
        "\n",
        "        # execute search\n",
        "        result = search.fit(X_train, y_train)\n",
        "\n",
        "        # get the best performing model fit on the whole training set\n",
        "        best_model = result.best_estimator_\n",
        "\n",
        "        # evaluate model on the hold out dataset\n",
        "        yEstimation = best_model.predict(X_test)\n",
        "\n",
        "        # evaluate the model\n",
        "        acc = accuracy_score(y_test, yEstimation.round())\n",
        "        f1score = f1_score(y_test, yEstimation,average='macro')\n",
        "\n",
        "        if (maxF1 < f1score):\n",
        "            maxF1 = f1score\n",
        "            superBestModel = best_model\n",
        "\n",
        "        # report progress\n",
        "        # print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
        "\n",
        "    # summarize the estimated performance of the model\n",
        "    return superBestModel\n",
        "\n",
        "# train the best model and evaluate its results \n",
        "def trainTheModelAndEvalute(X, y, model):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "  superBestModel = nestedCrossValidationAlgorithm(X, y, model)\n",
        "  superBestModel.fit(X_train, y_train)\n",
        "  y_bestEst = superBestModel.predict(X_test) #probabilities vector\n",
        "\n",
        "  # get the real classification labels\n",
        "  y_true = np.argmax(y_test) #y_test is a one hot vector\n",
        "  y_pred = np.argmax(y_test)\n",
        "\n",
        "  # Performance metrics for evaluation\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  TPR = (tp)/(tp + fn)\n",
        "  FPR = (fp)/(fp + tn)\n",
        "  Precision = precision_score(y_true, y_pred)\n",
        "  AUC = roc_auc_score(y_true, y_pred)\n",
        "  PR_curve = average_precision_score(y_true, y_pred)\n",
        "  \n",
        "  return accuracy, TPR, FPR, Precision, AUC, PR_curve\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzR7lwzI4q6u"
      },
      "source": [
        "# currentModel = getFixMatch_model(INIT_LEARNING_RATE = 0.03, batch_size = 256)\n",
        "\n",
        "# currentModel = getFixMatch_model()    # Augmentation feature\n",
        "\n",
        "NUM_CLASSES = 5\n",
        "numDatasets = 20\n",
        "\n",
        "for iDataset in range(numDatasets):\n",
        "  X, y = splitSubSetData(iDataset)\n",
        "  currentModel = getCNN_model(X)\n",
        "  superBestModel = nestedCrossValidationAlgorithm(X, y, currentModel)\n",
        "  accuracy, TPR, FPR, Precision, AUC, PR_curve = trainTheModelAndEvalute(X, y, superBestModel)\n",
        "  \n",
        "  print(\"iDataset: \", iDataset)\n",
        "  print(\"accuracy: \" , accuracy)\n",
        "  print(\"TPR: \" , TPR)\n",
        "  print(\"FPR: \" , FPR)\n",
        "  print(\"Precision: \" , Precision)\n",
        "  print(\"AUC: \" , AUC)\n",
        "  print(\"PR_curve: \" , PR_curve)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh2h5wvp12KF"
      },
      "source": [
        "## Significant test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edr_Qa4H1oEK"
      },
      "source": [
        "from scipy.stats import friedmanchisquare\n",
        "from scipy.stats import f_oneway\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "\n",
        "# compare distributions of the 3 different models\n",
        "def friedman_test(Alg1, Alg2, Alg3):\n",
        "  # compare samples\n",
        "  return friedmanchisquare(Alg1, Alg2, Alg3)\n",
        "\n",
        "\n",
        "# calculates the differences between the distributions in terms of p-value and mean difference\n",
        "def post_hoc_test(Alg1, Alg2, Alg3):\n",
        "  scores = Alg1\n",
        "  scores.extend(Alg2)\n",
        "  scores.extend(Alg3)\n",
        "  #create DataFrame to hold data\n",
        "  df = pd.DataFrame({'score': scores,group': np.repeat(['Alg1', 'Alg2', 'Alg3'], repeats=10)}) \n",
        "  # perform Tukey's test\n",
        "  tukey = pairwise_tukeyhsd(endog=df['score'], groups=df['group'], alpha=0.05)\n",
        "  #display results\n",
        "  print(tukey)\n",
        "\n",
        "# we have saved the results of each algorithm manually in excel, and load it from drive in order to calculate the significant tests\n",
        "df1 = pd.read_excel('/content/drive/MyDrive/MLAssignment/Alg1_results.xlsx', header=None).iloc[:,:-1]\n",
        "df2 = pd.read_excel('/content/drive/MyDrive/MLAssignment/Alg2_results.xlsx', header=None).iloc[:,:-1]\n",
        "df3 = pd.read_excel('/content/drive/MyDrive/MLAssignment/Alg3_results.xlsx', header=None).iloc[:,:-1]\n",
        "\n",
        "df1.columns= ['Dataset Name', 'Algorithm Name', 'Cross Validation [1-10]', 'HyperParamaters Values', 'ACC', 'TPR', 'FPR', 'Precision','AUC', 'PR-CURVE', 'Training Time', 'Inference Time']\n",
        "df2.columns= ['Dataset Name', 'Algorithm Name', 'Cross Validation [1-10]', 'HyperParamaters Values', 'ACC', 'TPR', 'FPR', 'Precision','AUC', 'PR-CURVE', 'Training Time', 'Inference Time']\n",
        "df3.columns= ['Dataset Name', 'Algorithm Name', 'Cross Validation [1-10]', 'HyperParamaters Values', 'ACC', 'TPR', 'FPR', 'Precision','AUC', 'PR-CURVE', 'Training Time', 'Inference Time']\n",
        "\n",
        "metric = 'ACC'\n",
        "# metric = 'AUC'\n",
        "\n",
        "# take the results of the metric in the dataset for each algorithm\n",
        "Alg1 = df1[metric].values.tolist()\n",
        "Alg2 = df2[metric].values.tolist()\n",
        "Alg3 = df3[metric].values.tolist()\n",
        "\n",
        "# apply the firdeman test\n",
        "stat, p = friedman_test(Alg1, Alg2, Alg3)\n",
        "\n",
        "# interpret - if the distributions are different then apply also post_hoc test to measure the difference\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "  print('Same distributions (fail to reject H0)')\n",
        "else:\n",
        "  print('Different distributions (reject H0)')\n",
        "  print(post_hoc_test(Alg1, Alg2, Alg3))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}